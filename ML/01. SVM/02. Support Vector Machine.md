

# **Support Vector Machine(SVM)**

<hr>



## What is SVM?

- **결정 경계(Decision Boundary)**, 즉 분류를 위한 기준 선을 정의하는 모델

  - **데이터 군으로부터 최대한 멀리 떨어지는 게 좋음.**

  ![img](https://i0.wp.com/hleecaster.com/wp-content/uploads/2020/01/svm01.png?fit=1024%2C806)

-  **Support Vectors는 결정 경계와 가까이 있는 데이터 포인트들을 의미**



## Margin

- **결정 경계와 서포트 벡터 사이의 거리**를 의미

  - 아래의 실선이 결정 경계
  - 점선으로부터 결정 경계까지의 거리가 margin임.

  - ![img](https://i0.wp.com/hleecaster.com/wp-content/uploads/2020/01/svm04.png?fit=1024%2C768)

-  **n개의 속성을 가진 데이터에는 최소 n+1개의 서포트 벡터가 존재**

  - 위 그림의 점선과 맞물린 점

- **SVM에서는 결정 경계를 정의하는 게 SV기 때문에 나머지 쓸 데 없는 데이터 포인트 들을 무시할 수 있음.**

  - 따라서, 매우 빠름.



## SVM Usage

```python
from sklearn.svm import SVC
classifier = SVC(kernel = 'linear')
training_points = [[1, 2], [1, 5], [2, 2], [7, 5], [9, 4], [8, 2]]
labels = [1, 1, 1, 0, 0, 0]
classifier.fit(training_points, labels) 

print(classifier.predict([[3, 2]]))
```



## Outlier(이상치)

- ### Hard Margin

  - 서포트 벡터와 결정 경계 사이의 거리가 매우 좁음.
  - 아웃라이어를 허용하지 않는 기준으로 결정 경계
  - **오버피팅(overfitting)** 문제 발생

- **Soft Margin**

  - 서포트 벡터와 결정 경계 사이의 거리가 멀어짐.

  - **언더피팅(underfitting)** 문제 발생

    ![img](https://i1.wp.com/hleecaster.com/wp-content/uploads/2020/01/svm06.png?fit=1024%2C768)



## Parameter C

-  **scikit-learn에서는 SVM 모델이 오류를 어느정도 허용할 것인지 파라미터 `C`를 통해 지정**(default = 1)

```python
classifier = SVC(C = 0.01)
```



## Parameter kernel

- **다항식(polynomial)** 커널

  -  2차원에서 x, y 좌표로 이루어진 점들을 아래와 같은 식에 따라 3차원으로 표현

    ![img](http://hleecaster.com/wp-content/uploads/2020/01/svm08.png)

  - **데이터를 더 높은 차원으로 변형하여 나타냄으로써 초평면(hyperplane)의 결정 경계를 얻을 수 있다.**

- **RBF 커널** 혹은 **가우시안 커널**

  -  2차원의 점을 무한한 차원의 점으로 변환
  - 시각화 어려움



## Parameter gamma

- **결정 경계를 얼마나 유연하게 그을 것인지**
  - 값을 **높이면** 학습 데이터에 많이 의존해서 **결정 경계를 구불구불** 긋게 된다. 이는 **오버피팅**을 초래
  - 값을 **낮추면** 학습 데이터에 별로 의존하지 않고 **결정 경계를 직선에 가깝게** 긋게 된다. 이는 **언더피팅** 발생

```python
classifier = SVC(kernel = "rbf", C = 2, gamma = 0.5)
```

