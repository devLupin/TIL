# 순환 신경망(Recurrent Neural Network, RNN)

<hr>

- 입력과 출력을 시퀸스(Sequence) 단위로 처리하는 시퀸스 모델



## 1. RNN

<hr>

- 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 보냄.
- x는 입력층의 입력 벡터, y는 출력층의 출력 벡터, 편향 b
- 셀(cell) : RNN에서 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드
  - 이전의 값을 기억하려고 하는 일종의 메모리 역할을 수행하므로 **메모리 셀**, **RNN 셀**이라고 표현
  - 각 시점(time step)에서 바로 이전 시점에서의 은닉층의 메모리 셀에서 나온 값을 자신의 입력으로 사용하는 재귀적 활동

![img](https://wikidocs.net/images/page/22886/rnn_image1_ver2.PNG)

- **은닉 상태(hidden state)** : 메모리 셀이 출력층 방향으로 또는 t+1 시점의 자신에게 보내는 값

![img](https://wikidocs.net/images/page/22886/rnn_image2_ver3.PNG)

- many-to-one 모델

  - 입력 문서가 긍정적인지 부정적인지 판별하는 감성 분류(sentiment classification), 정상 메일인지 스팸 메일인지 판별하는 스팸 메일 분류(spam detection) 
  - **텍스트 분류**

  ![img](https://wikidocs.net/images/page/22886/rnn_image3.5.PNG)

- many-to-many 모델
  - 입력 문장으로부터 대답 문장을 출력하는 챗복, 입력 문장으로부터 번역된 문장을 출력하는 번역기, **개체명 인식, 품사 태깅**과 같은 작업

![img](https://wikidocs.net/images/page/22886/rnn_image3.7.PNG)



## 2. RNN 수식

<hr>

- 현재 시점 t에서의 은닉 상태값을 ht라고 정의
- 은닉층의 메모리 셀은 ht를 계산하기 위해서 총 두 개의 가중치를 갖는다.
  - 입력층에서 입력값을 위한 가중치 Wx, 이전 시점 t-1의 은닉 상태값인 ht−1ht−1을 위한 가중치 Wh
  - ~~입력값과 은닉 상태값을 받는다?~~

![img](https://wikidocs.net/images/page/22886/rnn_image4_ver2.PNG)
$$
은닉층 : h_{t} = tanh(W_{x} x_{t} + W_{h}h_{t−1} + b)
\ \\
출력층 : y_{t} = f(W_{y}h_{t} + b)
\ \\
단 \ f는 비선형\ 활성화\ 함수\ 중\ 하나.
$$

- 각 벡터와 행렬의 크기

  - 단어 벡터의 차원 d, 은닉 상태의 크기 Dh
  - ![image-20210216192430481](C:\Users\devLupin\AppData\Roaming\Typora\typora-user-images\image-20210216192430481.png)
  - ex) 배치 크기 1, d = Dh = 5

  ![img](https://wikidocs.net/images/page/22886/rnn_images4-5.PNG)
  - 은닉층 ht를 계산하기 위한 활성화 함수로는 tanh가 사용되지만, ReLU로 바꿔 사용하는 경우도 있음.
  - 은닉층이 2개 이상인 경우 2개의 가중치는 서로 다름.



## 3. RNN 구현(Keras)

<hr>

```python
# RNN 층을 추가하는 코드.
model.add(SimpleRNN(hidden_size)) # 가장 간단한 형태

# 추가 인자를 사용할 때
model.add(SimpleRNN(hidden_size, input_shape=(timesteps, input_dim)))

# 다른 표기
model.add(SimpleRNN(hidden_size, input_length=M, input_dim=N))
# 단, M과 N은 정수
```

- hidden_size
  - 은닉상태 크기 정의
  - 메모리 셀이 다음 시점의 메모리 셀과 출력층으로 보내는 값의 크기(output_dim)와도 동일
  - 중소형 모델의 경우 보통 128, 256, 512, 1024등의 값을 가짐
- timesteps : 입력 시퀸스의 길이 또는 시점의 수
- input_dim : 입력의 크기

- RNN 층은 (batch_size, timesteps, input_dim) 크기의 3D 텐서를 입력으로 받는다.
  - batch_size : 한 번에 학습하는 데이터의 개수

![img](https://wikidocs.net/images/page/22886/rnn_image6between7.PNG)

- RNN 층의 결과는 하나의 은닉 상태 또는 다수의 은닉 상태를 리턴

![img](https://wikidocs.net/images/page/22886/rnn_image7_ver2.PNG)

- 메모리 셀의 최종 시점의 은닉 상태만을 리턴하고자 한다면 (batch_size, output_dim) 크기의 2D 텐서 리턴
- 메모리 셀의 각 time step의 은닉 상태값들을 모아서 전체 시퀸스를 리턴하고자 한다면 (batch_size, timesteps, output_dim) 크기의 3D 텐서 리턴
  - RNN 층의 return_sequences 매개 변수에 True를 설정하여 가능
  - output_dim은 hidden_size의 값으로 설정됨.
- ex) time_step = 3

![img](https://wikidocs.net/images/page/22886/rnn_image8_ver2.PNG)



## 4. 깊은 순환 신경망(Deep Recurrent Neural Network, Deep RNN)

<hr>

- RNN도 다수의 은닉층을 가질 수 있다.

```python
model = Sequential()

# 은닉층 2개 추가
model.add(SimpleRNN(hidden_size, return_sequences = True))
model.add(SimpleRNN(hidden_size, return_sequences = True))
```

![img](https://wikidocs.net/images/page/22886/rnn_image4.5_finalPNG.PNG)



## 5. 양방향 순환 신경망(Bidrectional Recurrent Neural Network)

<hr>

- 시점 t에서의 출력값을 예측할 때 이전 시점의 데이터뿐 아니라 이후 데이터로도 예측할 수 있다는 아이디어 기반

- 하나의 출력값을 예측하기 위해 기본적으로 두 개의 메모리 셀 사용

  1. **앞 시점의 은닉 상태(Forward States**를 전달 받아 현재의 은닉상태 계산(주황색 메모리 셀)
  2. **뒤 시점의 은닉 상태(Backward States)**를 전달 받아 현재의 은닉상태 계산(초록색 메모리 셀)

  - 이 두 개의 값 모두 출력층에서 전달 받아 출력값 예측을 위해 사용

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Bidirectional

model = Sequential()
model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True), input_shape=(timesteps, input_dim)))
```

![img](https://wikidocs.net/images/page/22886/rnn_image5_ver2.PNG)

- 깊은 양방향 RNN

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Bidirectional

model = Sequential()
model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True), input_shape=(timesteps, input_dim)))
model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True)))
model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True)))
model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True)))
```

![img](https://wikidocs.net/images/page/22886/rnn_image6_ver3.PNG)