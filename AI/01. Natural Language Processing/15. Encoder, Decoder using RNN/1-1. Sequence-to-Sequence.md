# 시퀀스-투-시퀀스(Sequence-to-Sequence, seq2seq)

<hr>

- 입력된 시퀸스로부터 다른 도메인의 시퀸스 출력

- 번역기에서 사용되는 대표적인 모델

- seq2seq 모델

  - 인코더, 디코더 두개의 아키텍처로 구성

  - 인코더와 디코더 아키텍처 내부는 RNN 아키텍처임.

  - 성능 문제로 바닐라 RNN이 아닌 LSTM 또는 GRU 셀로 구성

  - 인코더의 입력 문장의 단어 토큰화를 통해서 단어 단위로 쪼개지고 단어 토큰은 RNN의 각 시점의 입력이 됨.

  - 인코더 RNN

    - 컨텍스트 벡터(모든 단어를 입력 받은 후 **인코더 RNN 셀의 마지막 시점의 은닉 상태**)를 디코더 RNN 셀로 넘겨줌.
    - 컨텍스트 벡터는 **디코더 RNN 셀의 첫번째 은닉 상태로 사용**
    - 컨텍스트 벡터는 입력 문장의 모든 단어 토큰들의 정보를 요약해서 담고 있음.

  - 디코더 RNN

    - 디코더는 초기 입력으로 문장의 시작을 의미하는 심볼 <sos>(현재 t의 입력)와 인코더의 마지막 RNN 셀의 은닉 상태인 컨텍스트 벡터가 들어감
    - 등장 확률이 높은 단어 예측 하고 이는 다음 시점 t+1 RNN에서의 입력값이 됨.
    - 첫번째 시점의 디코더 RNN 셀은 예측된 단어를 다음 시점의 RNN 셀의 입력으로 사용하는 행위 반복
    - 문장의 끝을 의미하는 <eos>가 입력될 때 까지

  - seq2seq에서 사용되는 모든 단어들은 워드 임베딩을 통해 표현된 임베딩 벡터임.

  - seq2seq의 훈련 과정, 테스트 과정

    - 훈련 과정에서는 디코더에게 인코더가 보낸 컨텍스트 벡터와 출력 정답을 알려주면서 훈련
      - RNN의 모든 시점에 대해서 이전 시점의 예측값 대신 실제 값을 입력으로 주는 방법을 교사 강요라고 함.
        - 이전 시점의 디코더 셀의 예측이 틀렸고 이를 현재 시점의 디코더 셀의 입력으로 사용하면 현재 시점의 디코더 셀 예측도 잘못될 가능성이 높고 이는 연쇄 작용으로 디코더 전체의 예측을 어렵게 함.
    - 테스트 과정에서는 디코더는 오직 컨텍스트 벡터와 <sos>만을 입력으로 받은 후 다음에 올 단어 예측하고 그 단어를 다음 시점의 RNN 셀의 입력으로 넣는 행위 반복

    ![img](https://wikidocs.net/images/page/24996/%EB%8B%A8%EC%96%B4%ED%86%A0%ED%81%B0%EB%93%A4%EC%9D%B4.PNG)

- seq2seq 모델은 모든 단어들로부터 하나의 단어를 골라서 예측하는데 소프트맥스 함수를 사용

  - 디코더의 각 시점의 RNN 셀에서 출력 벡터가 나오면, 해당 벡터는 소프트 맥스 함수를 통해 시퀸스의 각 단어별 확률 값을 반환하고 디코더는 출력 단어를 결정함.

  ![img](https://wikidocs.net/images/page/24996/decodernextwordprediction.PNG)