# Transformer

<hr>



## 1. seq2seq 모델의 한계

<hr>

- 인코더-디코더 구조
- 인코더는 입력 시퀸스를 하나의 벡터 표현으로 압축
- 디코더는 벡터 표현을 통해 출력 시퀸스 제작
- 하나의 벡터로 압축하는 과정에서 입력 시퀸스의 정보가 일부 손실되고 이를 보정하기 위해 어텐션 사용



## 2. Transformer

<hr>

- RNN을 사용하지 않음.
- 기존의 seq2seq 같이 인코더에서 입력 시퀸스를 입력 받고 디코더에서 출력 시퀸스를 출력
- 인코더-디코더 단위가 N개 존재할 수 있음.

![img](https://wikidocs.net/images/page/31379/transformer1.PNG)

- 시작 심폴 <sos>를 입력받아 종료 심볼 <eos>가 나올 때까지 연산 진행
- 각 단어의 임베딩 벡터에서 조정된 값을 입력받음.

![img](https://wikidocs.net/images/page/31379/transformer4_final_final_final.PNG)



## 3. Main parameters of Transformer

<hr>

![Transformer main parameters.PNG](https://github.com/devLupin/TIL/blob/master/AI/01.%20Natural%20Language%20Processing/%23.%20image%20src/Transformer%20main%20parameters.PNG?raw=true)



## 4. Positional Encoding

<hr>

- 트랜스포머는 단어 입력을 순차적으로 받는 방식이 아니기 때문에 단어의 위치 정보를 알려줘야 함.

- 포지셔널 인코딩 : 단어의 위치 정보를 얻기 위해 각 단어의 임베딩 벡터에 위치 정보들을 더해 모델의 입력으로 사용

  - sin, cos 함수의 값을 임베딩 벡터에 더해 단어의 순서 정보를 더함.

    - 임베딩 벡터 내의 각 차원의 인덱스가 짝수인 경우 sin 함수 값, 홀수인 경우 cos 함수 값 사용

    $$
    PE_{(pos,\ 2i)}=sin(pos/10000^{2i/d_{model}})
    \ \\
    PE_{(pos,\ 2i+1)}=cos(pos/10000^{2i/d_{model}})
    $$

  - 임베딩 벡터와 포지셔널 인코딩의 덧셈은 임베딩 벡터가 모여 만들어진 문장 벡터 행렬, 포지셔널 인코딩 행렬의 덧셈 연산

- 각 임베딩 벡터에 포지셔널 인코딩값을 더하면 같은 단어라도 위치에 따라 트랜스포머의 입력으로 들어가는 임베딩 벡터의 값이 달라짐.

```python
class PositionalEncoding(tf.keras.layers.Layer):
  def __init__(self, position, d_model):
    super(PositionalEncoding, self).__init__()
    self.pos_encoding = self.positional_encoding(position, d_model)

  def get_angles(self, position, i, d_model):
    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))
    return position * angles

  def positional_encoding(self, position, d_model):
    angle_rads = self.get_angles(
        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],
        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],
        d_model=d_model)

    # 배열의 짝수 인덱스(2i)에는 사인 함수 적용
    sines = tf.math.sin(angle_rads[:, 0::2])

    # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용
    cosines = tf.math.cos(angle_rads[:, 1::2])

    angle_rads = np.zeros(angle_rads.shape)
    angle_rads[:, 0::2] = sines
    angle_rads[:, 1::2] = cosines
    pos_encoding = tf.constant(angle_rads) 	# 상수 값 생성
    pos_encoding = pos_encoding[tf.newaxis, ...]

    print(pos_encoding.shape)
    return tf.cast(pos_encoding, tf.float32)

  def call(self, inputs):
    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]
```

```python
# 문장의 길이 50, 임베딩 벡터의 차원 128
sample_pos_encoding = PositionalEncoding(50, 128)

plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')
plt.xlabel('Depth')
plt.xlim((0, 128))
plt.ylabel('Position')
plt.colorbar()
plt.show()
```



## 5. Attention

<hr>

- Self-Attention, Masked Decoder Self-Attention
  - Query, Key, Value의 벡터 출처가 동일
- Encoder-Decoder Attention
  - Query가 디코더 벡터, Key, Value(벡터 출처 동일)가 인코더 벡터

![img](https://wikidocs.net/images/page/31379/attention.PNG)

- 트랜스포머의 어텐션 병렬적(Multi-head) 수행

![img](https://wikidocs.net/images/page/31379/transformer_attention_overview.PNG)



## 6. Encoder

<hr>

- hyperparameter인 num_layers 개수의 인코더 층을 쌓음.
- 두 개의 서브층(Self-Attention, FFNN)으로 구성
  - 바로 위 그림의 노란 박스에 해당됨.



## 7. Self-Attention of Encoder

<hr>

### 1)  Self-Attention 의미 및 이점

- Attention
  - 주어진 쿼리에 대해 모든 키와의 유사도를 구함
  - 유사도를 가중치로 하여 맵핑된 각각의 값에 반영
  - 유사도가 반영된 값을 모두 가중합 하여 리턴
  - Q = Query : t 시점의 디코더 셀에서의 은닉 상태 
  - K = Keys : 모든 시점의 인코더 셀의 은닉 상태들
  - V = Values : 모든 시점의 인코더 셀의 은닉 상태들
- Self-Attention
  - 자기 자신에게 어텐션 수행
  - Query, Key, Value : 입력 문장의 모든 단어 벡터
  - 입력 문장 내의 단어들끼리 유사도를 구해, 대명사(it, that, etc.)가 어떤 단어를 의미하는 지 찾아낼 확률이 높아짐.

### 2)  Gain Q, K, V vectors

- 우선적으로, 각 단어 벡터들로부터 Q, K, V 벡터를 얻는 작업 진행

  - 각 벡터들은 d_model의 차원을 가지는 단어 벡터들보다 더 작은 차원을 가짐.
    - 여기서 더 작은 차원은 d_model / num_heads로 결정
    - 더 작은 벡터는 가중치 행렬을 곱해 완성됨.
      - 가중치 행렬은 d_model X (d_model / num_heads)의 크기

  ![img](https://wikidocs.net/images/page/31379/transformer11.PNG)

### 3)  Scaled dot-product Attention

- 모든 Q 벡터에 대해 반복

  - 모든 K 벡터에 대한 어텐션 스코어를 구함.
  - 이를 사용하여 모든 V 벡터를 가중합하여 어텐션 값 또는 컨텍스트 벡터를 구함.

- Scaled dot-product Attetion : 트랜스포머는 어텐션 함수에 특정 값을 나눈 함수를 사용

  - 두 벡터의 내적값을 스케일링하는 값으로 K 벡터의 차원을 나타내는 d_k에 루트를 씌운 √d_k 사용
  - d_k = d_model / num_heads

  $$
  score(q, k)=q⋅k/\sqrt{d_{k}}
  $$
  - Gain Attention score

  ![img](https://wikidocs.net/images/page/31379/transformer13.PNG)
  - Gain Attention Distribution

    - Attention score에 소프트맥스 함수를 사용
    - 각 벡터 V와 가중합하여 어텐션 값(Attention value) 또는 컨텍스트 벡터를 구함.

    ![img](https://wikidocs.net/images/page/31379/transformer14_final.PNG)

### 4) 행렬 연산으로 일괄  처리

- 상기 3번 과정은 행렬 연산을 이용해 일괄 계산 가능

1. 문장 행렬에 가중치 행렬을 곱하여 Q, K, V 행렬을 구함.

![img](https://wikidocs.net/images/page/31379/transformer12.PNG)

2. Q 행렬을 K 행렬을 전치한 행렬과 곱하면 각 단어의 Q, K 벡터의 내적이 각 행렬의 원소가 되는 행렬이 나옴.

![img](https://wikidocs.net/images/page/31379/transformer15.PNG)

3. 위의 결과 행렬의 값에 √(d_k)를 나누어주면 각 행과 열이 어텐션 스코어 값을 가지는 행렬이 됨.

4. 어텐션 분포를 구하고, 이를 사용하여 모든 단어에 대한 어텐션 값을 구함.

   - 어텐션 스코어 행렬에 소프트 맥스 함수 적용 후, V 행렬을 곱함.

   ![img](https://wikidocs.net/images/page/31379/transformer16.PNG)

- 최종 수식화
  - 입력 문장의 길이를 seq_len이라고 하면, 문장 행렬 크기는 (seq_len, d_model)
  - 문장 행렬에 3개의 가중치 행렬을 곱해서 Q, K, V 행렬을 만듦.
  - 아래의 식을 적용한 어텐션 값 행렬의 크기는 (seq_len, d_v)
    - d_v는 V 벡터의 크기

$$
Attention(Q, K, V) = softmax({QK^T\over{\sqrt{d_k}}})V
$$

### 5) Scaled dot-product Attention 구현

```python
def scaled_dot_product_attention(query, key, value, mask):
  # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)
  # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)
  # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)
  # padding_mask : (batch_size, 1, 1, key의 문장 길이)

  # Q와 K의 곱. 어텐션 스코어 행렬
  matmul_qk = tf.matmul(query, key, transpose_b=True)

  # 스케일링
  # dk의 루트값으로 나눔.
  depth = tf.cast(tf.shape(key)[-1], tf.float32)
  logits = matmul_qk / tf.math.sqrt(depth)

  # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값 삽입
  # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 됨.
  if mask is not None:
    logits += (mask * -1e9)

  # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행
  # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)
  attention_weights = tf.nn.softmax(logits, axis=-1)

  # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)
  output = tf.matmul(attention_weights, value)

  return output, attention_weights
```

### 5-1) Padding Mask

```python
def scaled_dot_product_attention(query, key, value, mask):
# ... 중략 ...
    logits += (mask * -1e9) # 어텐션 스코어 행렬인 logits에 mask*-1e9 값을 더해주고 있다.
#... 중략 ...
```

- 이는 입력 문장의 <pad>토큰을 어텐션에서 제외시키기 위한 연산
  - <pad>는 실질적인 의미를 가진 단어가 아님
  - Key에 <pad> 토큰이 존재하면 이에 대해 유사도를 구하지 않도록 Masking 함.
  - 어텐션 스코어 행렬의 마스킹 위치에 매우 작은 음수 값(-INF에 가까운 수)을 넣어주는 식으로 구현
- 어텐션 스코어 행렬이 소프트맥스 함수를 지난 후 <pad>위치의 값은 0에 가까운 값이 되어 단어 간 유사도에 반영되지 않음.
- 입력된 정수 시퀸스에서 패딩 토큰의 인덱스인지, 아닌지를 판별하는 함수를 구현

```python
def create_padding_mask(x):
  mask = tf.cast(tf.math.equal(x, 0), tf.float32)
  # (batch_size, 1, 1, key의 문장 길이)
  return mask[:, tf.newaxis, tf.newaxis, :]
```

- 테스트 코드

```python
# 임의의 Query, Key, Value인 Q, K, V 행렬 생성
np.set_printoptions(suppress=True)
temp_k = tf.constant([[10,0,0],
                      [0,10,0],
                      [0,0,10],
                      [0,0,10]], dtype=tf.float32)  # (4, 3)

temp_v = tf.constant([[   1,0],
                      [  10,0],
                      [ 100,5],
                      [1000,6]], dtype=tf.float32)  # (4, 2)
temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)

# 함수 실행
temp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)
print(temp_attn) # 어텐션 분포(어텐션 가중치의 나열)
print(temp_out) # 어텐션 값
```

### 6) Multi-head Attention

- d_model / num_heads의 차원을 가지는 Q, K, V에 대해 하이퍼파라미터 num_heads개의 병렬 어텐션 수행
- 이 때 각각의 병렬 수행된 어텐션 값 행렬을 어텐션 헤드라고 함.
- 병렬로 수행하여 다른 시각으로 정보 수집
  - 이 때의 Q, K, V에 대한 가중치 행렬 값은 N개의 어텐션 헤드마다 전부 상이함.
- 병렬 어텐션을 모두 수행하면 모든 어텐션 헤드를 concatenate
  - 모든 어텐션 헤드의 크기는 (seq_len, d_model)
- 연결된 모든 어텐션 헤드는 또 다른 가중치 행렬 W0과 곱함. 이것이 멀티-헤드 어텐션의 최종 결과물
- 첫 번째 서브층인 멀티-헤드 어텐션, 두 번째 서브층인 포지션 와이즈 피드 포워드 신경망을 지나면서 인코더의 입력으로 들어올 때 행렬 크기는 유지되어야 함.

### 7) Multi-head Attention 구현

- 멀티 헤드 어텐션에서는 가중치 행렬 WQ, WK, WV와 어텐션 헤드들을 연결 후에 곱해주는 WO 행렬이 나옴.
- 구현 상에서 가중치 행렬을 곱하는 것은 밀집층을 지나게 함으로써 구현
- Multi-head Attention의 5가지 파트
  1. WQ, WK, WV에 해당하는 d_model 크기의 밀집층(Dense layer)을 지나게 함.
  2. 지정된 헤드 수(num_heads)만큼 나눈다(split).
  3. Scaled dot-product Attention
  4. 나눠진 헤드들을 concatenate
  5. WO에 해당하는 밀집층을 지나게 함.

```python
class MultiHeadAttention(tf.keras.layers.Layer):

  def __init__(self, d_model, num_heads, name="multi_head_attention"):
    super(MultiHeadAttention, self).__init__(name=name)
    self.num_heads = num_heads
    self.d_model = d_model

    assert d_model % self.num_heads == 0

    # d_model을 num_heads로 나눈 값.
    # 논문 기준 : 64
    self.depth = d_model // self.num_heads

    # WQ, WK, WV에 해당하는 밀집층 정의
    self.query_dense = tf.keras.layers.Dense(units=d_model)
    self.key_dense = tf.keras.layers.Dense(units=d_model)
    self.value_dense = tf.keras.layers.Dense(units=d_model)

    # WO에 해당하는 밀집층 정의
    self.dense = tf.keras.layers.Dense(units=d_model)

  # num_heads 개수만큼 q, k, v를 split하는 함수
  def split_heads(self, inputs, batch_size):
    inputs = tf.reshape(
        inputs, shape=(batch_size, -1, self.num_heads, self.depth))
    return tf.transpose(inputs, perm=[0, 2, 1, 3])
	"""
	perm 인자는 텐서의 차원을 변경하는 인자 값
	ex)
	origin dim = [1, 2, 3] 일 때, perm[2, 1, 0]을 주면
	dim = [3, 2, 1]로 바뀐 행렬 반환
	"""

  def call(self, inputs):
    query, key, value, mask = inputs['query'], inputs['key'], inputs[
        'value'], inputs['mask']
    batch_size = tf.shape(query)[0]

    # 1. WQ, WK, WV에 해당하는 밀집층 지나기
    # q : (batch_size, query의 문장 길이, d_model)
    # k : (batch_size, key의 문장 길이, d_model)
    # v : (batch_size, value의 문장 길이, d_model)
    # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.
    query = self.query_dense(query)
    key = self.key_dense(key)
    value = self.value_dense(value)

    # 2. 헤드 나누기
    # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)
    # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)
    # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)
    query = self.split_heads(query, batch_size)
    key = self.split_heads(key, batch_size)
    value = self.split_heads(value, batch_size)

    # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.
    # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)
    scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)
    # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)
    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])

    # 4. 헤드 연결(concatenate)하기
    # (batch_size, query의 문장 길이, d_model)
    concat_attention = tf.reshape(scaled_attention,
                                  (batch_size, -1, self.d_model))

    # 5. WO에 해당하는 밀집층 지나기
    # (batch_size, query의 문장 길이, d_model)
    outputs = self.dense(concat_attention)

    return outputs
```



## 8.  Position-wise FFNN

<hr>

- FC FFNN임.
- x는 멀티 헤드 어텐션의 결과인 (seq_len, d_model)의 크기를 가지는 행렬, 가중치 행렬 W1은 (d_model, d_ff)의 크기, W2는 (d_ff, d_model)의 크기(d_ff는 은닉층의 크기임.)

$$
FFNN(x) = MAX(0, x{W_{1}} + b_{1}){W_2} + b_2
$$

- 매개변수 W1, b1, W2, b2는 하나의 인코더 층 내에서는 동일하게 사용되지만, 층마다는 다른 값을 가짐.

- 각 벡터들(실제로는 행렬)이 인코더 내 서브층인 멀티 헤드 어텐션 층을 지나 FFNN을 통과함.

  - FFNN은 두 번째 서브층인 Position-wise FFNN을 의미

- 두 번재 서브층을 지난 인코더의 최종 출력은 인코더의 입력 크기인 (seq_len, d_model)의 크기 유지

- 구현

  ```python
    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)
    outputs = tf.keras.layers.Dense(units=d_model)(outputs)
  ```



## 9. Residual connnection(잔차 연결) and Layer Normalization(정규화)

<hr>

- 트랜스 포머에서는 멀티 헤드 어텐션 층, Position-wise FFNN층을 가진 인코더에 추가적으로 사용하는 기법
- Add & Norm

![img](https://wikidocs.net/images/page/31379/transformer21.PNG)

### 1) Residual connection

- 서브층의 입력과 출력을 더하는 것
  - 서브층의 입/출력은 동일한 차원을 갖고 있으므로, 덧셈 연산 가능

### 2) Layer Normalization

- Residual connnection 이후 진행
- **텐서의 마지막 차원에 대한 평균과 분산을 구하고, 이를 어떤 수식을 통해 값을 정규화하여 학습**을 도움.
  - 텐서의 마지막 차원은 d_model

1. 평균 μ과 분산 σ^2을 구함.
2. 층 정규화 수행 후 벡터 x_i(화살표 방향의 벡터)는 ln_i라는 벡터로 정규화 됨.

![img](https://wikidocs.net/images/page/31379/layer_norm_new_2_final.PNG)

- Layer Normalization

  - 평균과 분산을 통한 정규화

    - x_i는 벡터, 평균과 분산은 스칼라
    - 벡터 x_i의 각 차원을 k라고 할 때, x_i,k의 정규화 수식
      - ϵ(입실론)은 분모가 0이 되는 것을 방지

    $$
    \hat{x}_{i, k} = \frac{x_{i, k}-μ_{i}}{\sqrt{σ^{2}_{i}+\epsilon}}
    $$

  - γ(감마)와 β(베타)의 도입

    - 이들의 초기 값은 각각 1, 0
    - γ, β는 학습 가능한 파라미터

    $$
    ln_{i} = γ\hat{x}_{i}+β = LayerNorm(x_{i})
    $$

- 케라스에서는 LayerNormalization()이라는 함수 형태로 제공됨.



## 10. 인코더 구현

<hr>

- 아래는 하나의 인코더 층을 구현하는 코드
- 실제로는 num_layers의 수만큼 인코더 층을 사용하므로 이를 여러번 쌓는 코드 별도 구현 필요

```python
def encoder_layer(dff, d_model, num_heads, dropout, name="encoder_layer"):
  inputs = tf.keras.Input(shape=(None, d_model), name="inputs")

  # 인코더는 패딩 마스크 사용
  # 입력 문장에 패딩이 있을 수 있으므로, 어텐션 시 패딩 토큰 제외
  padding_mask = tf.keras.Input(shape=(1, 1, None), name="padding_mask")

  # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)
  attention = MultiHeadAttention(
      d_model, num_heads, name="attention")({
          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V
          'mask': padding_mask # 패딩 마스크 사용
      })

  # Dropout + Residual connection, Layer Normalization
  attention = tf.keras.layers.Dropout(rate=dropout)(attention)
  attention = tf.keras.layers.LayerNormalization(
      epsilon=1e-6)(inputs + attention)

  # Position-wise FFNN (두번째 서브층)
  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)
  outputs = tf.keras.layers.Dense(units=d_model)(outputs)

  # Dropout + Residual connection, Layer Normalization
  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)
  outputs = tf.keras.layers.LayerNormalization(
      epsilon=1e-6)(attention + outputs)

  return tf.keras.Model(
      inputs=[inputs, padding_mask], outputs=outputs, name=name)
```



## 11. 인코더 쌓기

<hr>

- 인코더 층을 num_layers개만큼 쌓고, 마지막 인코더 층에서 얻는 (seq_len, d_model) 크기의 행렬을 디코더의 입력으로 사용해야 함.

```python
def encoder(vocab_size, num_layers, dff,
            d_model, num_heads, dropout,
            name="encoder"):
  inputs = tf.keras.Input(shape=(None,), name="inputs")

  # 인코더는 패딩 마스크 사용
  padding_mask = tf.keras.Input(shape=(1, 1, None), name="padding_mask")

  # 포지셔널 인코딩 + 드롭아웃
  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)
  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))
  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)
  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)

  # 인코더를 num_layers개 쌓기
  for i in range(num_layers):
    outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,
        dropout=dropout, name="encoder_layer_{}".format(i),
    )([outputs, padding_mask])

  return tf.keras.Model(
      inputs=[inputs, padding_mask], outputs=outputs, name=name)
```



## 12. From Encoder to Decoder

<hr>

- 인코더는 총 num_layersnum_layers만큼의 층 연산을 순차적으로 한 후에 마지막 층의 인코더의 출력을 디코더에게 전달
- 디코더 연산이 시작되어 디코더 또한 총 num_layersnum_layers만큼의 연산
  - 인코더가 보낸 출력을 각 디코더 층 연산에 사용

![img](https://wikidocs.net/images/page/31379/transformer_from_encoder_to_decoder.PNG)



## 13. 디코더의 첫 번째 서브층 : Self Attention, look-ahead mask

<hr>

- 임베딩 층과 포지셔널 인코딩을 거친 후의 문장 행렬이 입력됨.
- 교사 강요(Teacher Forcing)을 사용하여 훈련되므로, 번역할 문장 또한 <sos>같은 토큰을 한 번에 입력 받음.
- 디코더는 입력 문장으로부터 각 시점의 단어를 예측하도록 훈련됨.

- 현재 시점의 단어를 예측 할 때, 입력 문장 행렬로부터 미래 시점의 단어까지도 참고할 수 있는 현상 발생
- 현재 시점의 예측에서 현재 시점보다 미래에 있는 단어들을 참고하지 못하도록 look-ahead mask 도입
  - 디코더의 첫 번째 서브층에서 이루어짐.
  - 디코더, 인코더의 멀티 헤드 셀프 어텐션 층은 동일한 연산을 수행
    - 다른 점은 디코더는 어텐션 스코어 행렬에서 마스킹을 적용함.

1. 셀프 어텐션을 통해 어텐션 스코어 행렬을 얻음.

![img](https://wikidocs.net/images/page/31379/decoder_attention_score_matrix.PNG)

2. 자신보다 미래에 있는 단어들은 참고하지 못하도록 마스킹
   - 마스킹 된 후 어텐션 스코어 행렬의 각 행을 보면 **자기 자신과 그 이전 단어들만을 참고**

![img](https://wikidocs.net/images/page/31379/%EB%A3%A9%EC%96%B4%ED%97%A4%EB%93%9C%EB%A7%88%EC%8A%A4%ED%81%AC.PNG)

- look-ahead mask는 패딩 마스크와 마찬가지로 scale dot-product attention 함수에 mask라는 인자로 전달됨.

- 트랜스포머의 세 가지 멀티 헤드 어텐션 모두 내부에서 스케일드 닷 프로덕트 어텐션 함수 호출
  - 인코더의 셀프 어텐션 : 패딩 마스크를 전달
  - 디코더의 첫번째 서브층인 마스크드 셀프 어텐션 : 룩-어헤드 마스크를 전달
    - 룩-어헤드 마스크는 패딩 마스크를 포함하도록 구현
  - 디코더의 두번째 서브층인 인코더-디코더 어텐션 : 패딩 마스크를 전달
- look-ahead mask는 마스킹을 하고자 하는 위치에는 1, 그렇지 않으면 0을 리턴

```python
# 디코더의 첫번째 서브층(sublayer)에서 미래 토큰을 Mask하는 함수
def create_look_ahead_mask(x):
  seq_len = tf.shape(x)[1]
  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)
  padding_mask = create_padding_mask(x) # 패딩 마스크도 포함
  return tf.maximum(look_ahead_mask, padding_mask)
```



## **14. 디코더의 두번째 서브층 : Encoder-Decoder Attention**

<hr>

- 두 번째 서브층을 확대해보면, 두 개의 값이 입력됨.
  - 각각 Key, Value를 의미
  - 인코더의 마지막 층에서 온 행렬로부터 얻음.
- Query는 디코더의 첫 번째 서브층의 결과 행렬로부터 얻음.

![img](https://wikidocs.net/images/page/31379/%EB%94%94%EC%BD%94%EB%8D%94%EB%91%90%EB%B2%88%EC%A7%B8%EC%84%9C%EB%B8%8C%EC%B8%B5.PNG)



## 15. 디코더 구현

<hr>

- 세 개의 서브층으로 구성
  - 1, 2번째 서브층은 멀티 헤드 어텐션
  - 첫번째 서브층은 mask의 인자 값으로 look_ahead_mask
    - 첫번째 서브층은 마스크드 셀프 어텐션을 수행하기 때문
  - 두번째 서브층은 mask의 인자 값으로 padding_mask
  - 서브층 연산 후에 드롭 아웃, 잔차 연결, 층 정규화 수행
- 하나의 디코더에 대한 코드

```python
def decoder_layer(dff, d_model, num_heads, dropout, name="decoder_layer"):
  inputs = tf.keras.Input(shape=(None, d_model), name="inputs")
  enc_outputs = tf.keras.Input(shape=(None, d_model), name="encoder_outputs")

  # 룩어헤드 마스크(첫번째 서브층)
  look_ahead_mask = tf.keras.Input(
      shape=(1, None, None), name="look_ahead_mask")

  # 패딩 마스크(두번째 서브층)
  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')

  # 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션)
  attention1 = MultiHeadAttention(
      d_model, num_heads, name="attention_1")(inputs={
          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V
          'mask': look_ahead_mask # 룩어헤드 마스크
      })

  # 잔차 연결과 층 정규화
  attention1 = tf.keras.layers.LayerNormalization(
      epsilon=1e-6)(attention1 + inputs)

  # 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션)
  attention2 = MultiHeadAttention(
      d_model, num_heads, name="attention_2")(inputs={
          'query': attention1, 'key': enc_outputs, 'value': enc_outputs, # Q != K = V
          'mask': padding_mask # 패딩 마스크
      })

  # 드롭아웃 + 잔차 연결과 층 정규화
  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)
  attention2 = tf.keras.layers.LayerNormalization(
      epsilon=1e-6)(attention2 + attention1)

  # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)
  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)
  outputs = tf.keras.layers.Dense(units=d_model)(outputs)

  # 드롭아웃 + 잔차 연결과 층 정규화
  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)
  outputs = tf.keras.layers.LayerNormalization(
      epsilon=1e-6)(outputs + attention2)

  return tf.keras.Model(
      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],
      outputs=outputs,
      name=name)
```



## 16. 디코더 쌓기

<hr>

```python
def decoder(vocab_size, num_layers, dff,
            d_model, num_heads, dropout,
            name='decoder'):
  inputs = tf.keras.Input(shape=(None,), name='inputs')
  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')

  # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.
  look_ahead_mask = tf.keras.Input(
      shape=(1, None, None), name='look_ahead_mask')
  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')

  # 포지셔널 인코딩 + 드롭아웃
  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)
  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))
  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)
  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)

  # 디코더를 num_layers개 쌓기
  for i in range(num_layers):
    outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,
        dropout=dropout, name='decoder_layer_{}'.format(i),
    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])

  return tf.keras.Model(
      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],
      outputs=outputs,
      name=name)
```



## 17. 트랜스포머 구현

<hr>

- 인코더의 출력은 디코더에서 인코더-디코더 어텐션에서 사용되기 위해 디코더로 전달
- 디코더의 끝단에 다중 클래스 분류 문제 해결을 위해 vocab_size만큼의 뉴런을 가지는 신경망 추가

```python
def transformer(vocab_size, num_layers, dff,
                d_model, num_heads, dropout,
                name="transformer"):

  # 인코더의 입력
  inputs = tf.keras.Input(shape=(None,), name="inputs")

  # 디코더의 입력
  dec_inputs = tf.keras.Input(shape=(None,), name="dec_inputs")

  # 인코더의 패딩 마스크
  enc_padding_mask = tf.keras.layers.Lambda(
      create_padding_mask, output_shape=(1, 1, None),
      name='enc_padding_mask')(inputs)

  # 디코더의 룩어헤드 마스크(첫번째 서브층)
  look_ahead_mask = tf.keras.layers.Lambda(
      create_look_ahead_mask, output_shape=(1, None, None),
      name='look_ahead_mask')(dec_inputs)

  # 디코더의 패딩 마스크(두번째 서브층)
  dec_padding_mask = tf.keras.layers.Lambda(
      create_padding_mask, output_shape=(1, 1, None),
      name='dec_padding_mask')(inputs)

  # 인코더의 출력은 enc_outputs. 디코더로 전달된다.
  enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,
      d_model=d_model, num_heads=num_heads, dropout=dropout,
  )(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크

  # 디코더의 출력은 dec_outputs. 출력층으로 전달된다.
  dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,
      d_model=d_model, num_heads=num_heads, dropout=dropout,
  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])

  # 다음 단어 예측을 위한 출력층
  outputs = tf.keras.layers.Dense(units=vocab_size, name="outputs")(dec_outputs)

  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)
```



## 18. 트랜스포머 하이퍼파라미터

<hr>

- 단어 집합의 크기로부터 룩업 테이블을 수행할 임베딩 테이블, 포지셔널 인코딩 행렬의 행의 크기 결정 가능



## 19. 손실 함수 정의

<hr>

- 다중 클래스 분류 문제라면 크로스 엔트로피 함수를 손실 함수로 정의

```python
def loss_function(y_true, y_pred):
  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))

  loss = tf.keras.losses.SparseCategoricalCrossentropy(
      from_logits=True, reduction='none')(y_true, y_pred)

  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)
  loss = tf.multiply(loss, mask)

  return tf.reduce_mean(loss)
```



## 20. 학습률

<hr>

- 학습률은 고정된 값을 유지하는 것이 아닌, 학습 경과에 따라 변하도록 설계

$$
\Large{lrate = d_{model}^{-0.5} × min(\text{step_num}^{-0.5},\ \text{step_num} × \text{warmup_steps}^{-1.5})}
$$

