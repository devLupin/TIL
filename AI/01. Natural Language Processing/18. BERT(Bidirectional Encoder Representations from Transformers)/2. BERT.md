# **BERT(Bidirectional Encoder Representations from Transformers)**

<hr>



## 1. 개요

<hr>

- 트랜스포머를 이용하여 구현됨.
- 위키피디아(25억 단어), BooksCorpus(8억 단어)와 같은 레이블이 없는 텍스트 데이터로 사전 훈련된 언어 모델
- BERT의 높은 성능에 대한 방법
  - 레이블이 없는 방대한 데이터로 사전 훈련된 모델을 레이블이 있는 다른 작업에서 추가 훈련
  - 이와 함께 하이퍼파라미터를 재조정하여 모델 사용
    - Fine-tuning : 다른 작업에 대해 파라미터 재조정을 위한 추가 훈련 과정



## 2. BERT의 크기

<hr>

- BERT는 트랜스포머의 인코더를 쌓아올린 구조

- Base 버전은 12개, Large 버전은 24개를 쌓아 올림.

  - Large 버전은 Base 버전보다 d_model의 크기, 셀프 어텐션 헤드의 수가 더 큼.
  - 인코더 층의 수 L, d_model의 크기 D, 셀프 어텐션 헤드의 수 A 일 때
    - BERT-Base : L=12, D=768, A=12 : 110M개의 파라미터
    - BERT-Large : L=24, D=1024, A=16 : 340M개의 파라미터

  ![img](https://wikidocs.net/images/page/35594/bartbase%EC%99%80large.PNG)



## 3. 문맥을 반영한 임베딩(Contextual Embedding)

<hr>

- BERT는 문맥을 반영한 임베딩 사용
- BERT의 입력은 임베딩 층을 지난 임베딩 벡터
  - 모든 단어들은 d_model 차원의 임베딩 벡터가 되어 입력으로 사용됨.
- BERT는 내부적인 연산 후 각 단어에 대해 d_model 차원의 벡터 출력
  - 출력 임베딩은 문장의 문맥을 모두 참고한 문맥을 반영한 임베딩
- 하나의 단어가 모든 단어를 참고하는 연산은 BERT의 셀프 어텐션 헤드의 수의 층에서 연산이 이뤄짐.
  - 셀프 어텐션 헤드 수의 층을 지난 후 출력 임베딩 확보
- BERT의 첫번째 층의 출력 임베딩은 BERT의 두번째 층의 입력 임베딩
- 내부적으로 각 층마다 멀티 헤드 셀프 어텐션, 포지션 와이즈 피드 포워드 신경망 수행

![img](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC4.PNG)



## 4. 서브워드 토크나이저 : WordPiece

<hr>

- BERT는 단어보다 더 작은 단위로 쪼개는 서브워드 토크나이저 사용

- 서브워드 토크나이저

  - 자주 등장하는 단어는 그대로 단어 집합에 추가
  - 그렇지 않은 단어는 더 작은 단위인 서브워드로 분리되어 서브워드들이 단어 집합에 추가됨.
  - 단어집합을 기반으로 토큰화 수행

- BERT의 토큰화 수행 방식

  - 사전에 훈련 데이터로부터 만들어진 단어 집합 필요

  1. 단어 집합에 토큰 존재
     - 토큰 분리 안함.
  2. 단어 집합에 토큰 없음
     - 해당 토큰을 서브워드로 분리
     - 해당 토큰의 첫번째 서브워드를 제외한 나머지 서브워드들은 앞에 "##"를 붙인 것을 토큰으로 함.
       - ##은 단어의 중간에 등장하는 서브워드라는 것을 표기하기 위함.

  - 서브워드 토크나이저의 경우 해당 단어가 단어 집합에 존재하지 않으면 해당 단어를 더 쪼개려고 시도

- BERT에서 사용되는 특별 토큰, 그에 맵핑되는 정수

  - [PAD] - 0
  - [UNK] - 100
  - [CLS] - 101
  - [SEP] - 102
  - [MASK] - 103

  ```python
  with open('vocabulary.txt', 'w') as f:
    for token in tokenizer.vocab.keys():
      f.write(token + '\n')
      
  df = pd.read_fwf('vocabulary.txt', header=None)
  df.loc[102].values[0] 	# 102번 토큰 출력 예시
  ```



## 5. Position Embedding

<hr>

- 위치 정보를 학습을 통해서 얻는 방법

- 포지션 임베딩 사용 방법

  - WordPiece Embedding은 이미 알고 있는 단어 임베딩으로 실질적인 입력임.
  - 입력에 포지션 임베딩을 통해서 위치 정보를 더해줌.

  ![img](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC5.PNG)

- 아이디어

  - 위치 정보를 위한 임베딩 층을 하나 더 사용
  - ex) 문장의 길이가 4라면, 4개의 포지션 임베딩 벡터 학습 후 BERT의 입력마다 포지션 임베딩 벡터를 더함.

- 결론적으로 BERT에서는 세 개의 임베딩 층 사용

  - 단어집합의 단어 벡터를 위한 임베딩 층
  - 문장의 최대 길이가 N일 때 N개의 포지션 벡터를 위한 임베딩 층
  - 두 개의 문장을 구분하기 위한 임베딩 층



## 6. Pre-training of BERT

<hr>

- ELMo
  - 정방향 LSTM, 역방향 LSTM을 각각 훈련시키는 방식의 양방향 언어 모델
- GPT-1
  - 트랜스포머의 디코더를 이전 단어들로부터 다음 단어를 예측하는 방식의 단방향 언어 모델
- BERT
  - 마스크드 언어 모델을 통해 양방향성을 얻음.
  - BERT의 사전 훈련 방법
    - 마스크드 언어 모델(Masked Language Model, MLM)
    - 다음 문장 예측(Next sentence prediction, NSP)

![img](https://wikidocs.net/images/page/35594/bert-openai-gpt-elmo-%EC%B6%9C%EC%B2%98-bert%EB%85%BC%EB%AC%B8.png)

### 1) Masked Language Model, MLM

- 사전 훈련을 위해 인공 신경망의 입력 텍스트의 15%의 단어를 랜덤으로 마스킹
- 이후 마스킹 된 단어를 신경망을 통해 예측
- 15%의 단어들은 아래 비율로 규칙이 적용됨.
- '미변경 후 예측'의 경우 단어가 변경되지 않았지만 이 단어가 원래 단어인지 변경된 단어인지 알 수 없음.

![img](https://wikidocs.net/images/page/115055/%EC%A0%84%EC%B2%B4%EB%8B%A8%EC%96%B4.PNG)

- 출력층에 있는 다른 위치의 벡터들은 예측과 학습에 사용되지 않고, 오직 마스킹 된 단어 위치의 출력층 벡터만이 사용됨.
  - BERT의 손실 함수에서 다른 위치에서의 예측 무시
- 출력층의 단어 집합 크기의 밀집층에 소프트 맥스 함수가 사용된 1개의 층을 사용하여 원래 단어 예측

### 2) Next Sentence Prediction, NSP

- 두 개의 문장을 주어주고 후에 이 문장이 이어지는 문장인지 아닌지 맞추는 방식으로 훈련
- 실제 이어지는 두 개의 문장, 랜덤으로 이어붙인 두 개의 문장을 5:5 비율로 주고 훈련
  - 두 개의 문장이 끝나는 각 지점에 [SEP] 토큰을 붙임.
    - 각 문장은 다수의 문장 단위일 수 있음.
  - 이어붙인 두 개의 문장의 시작점에 [CLS] 토큰을 붙임
    - 이진 분류 문제를 풀게 하기 위함.
  - 경우에 따라 두 개의 문장을 입력받을 필요가 없을 수 있음.
    - 한 개의 문서에 대해서만 분류하는 경우, BERT의 전체 입력에 Sentence 0 임베딩만을 더함.
- 마스크드 언어 모델, 다음 문장 예측은 loss를 합하여 학습이 동시에 이뤄짐.
- NSP를 학습하는 이유
  - BERT 태스크 중에서는 두 문장의 관계를 이해하는 것이 중요한 게 있음.
  - QA(Question Answering), NLI(Natural Language Inference) 등



## 7. Segment Embedding

<hr>

- 문장 구분을 위한 임베딩 층
  - 첫 번째 문장에는 Sentence 0 임베딩
  - 두 번째 문장에는 Sentence 1 임베딩
  - 0, 1 임베딩을 더해주는 방식
  - 임베딩 벡터는 두 개만 사용됨.
- BERT의 3개의 임베딩 층
  - WordPiece Embedding : 실질적인 입력이 되는 워드 임베딩. 임베딩 벡터의 종류는 단어 집합의 크기
  - Position Embedding : 위치 정보를 학습하기 위한 임베딩. 임베딩 벡터의 종류는 문장의 최대 길이
  - Segment Embedding : 두 개의 문장을 구분하기 위한 임베딩. 임베딩 벡터의 종류는 문장의 최대 개수



## 8. BERT를 Fine-tuning

<hr>

- 풀고자 하는 태스크의 데이터를 추가로 학습시켜 테스트하는 단계
- 실질적으로 태스크에 BERT를 사용하는 단계

### 1) Single Text Classification

- 하나의 문서에 대한 텍스트 분류 유형
- 입력된 문서에 대해서 분류 하는 유형
- 문서 시작에 [CLS] 토큰 입력
- 텍스트 분류 문제를 풀기 위해 [CLS] 토큰 위치의 출력층에서 밀집층 또는 FC라고 불리는 층을 추가하여 분류에 대한 예측

### 2) Tagging about one text

- 대표적으로 문장의 각 단어에 품사 태깅(품사 태깅), 개체 태깅(개체명 인식)이 있음.
- 출력층에서 입력 텍스트의 각 토큰의 위치에 밀집층을 사용하여 분류에 대한 예측

### 3) Text Pair Classification or Regression

- 대표적으로 자연어 추론(Natural language inference)
  - 두 문장이 주어지면, 하나의 문장이 다른 문장과 어떤 관계에 있는지 분류하는 문제
  - 모순 관계(contradiction), 함의 관계(entailment), 중립 관계(neutral)
- 입력 텍스트가 1개가 아니므로, 텍스트 사이에 [SEP] 토큰 삽입
- Sentence 0, 1 세그먼트 임베딩을 모두 사용하여 문서 구분

### 4) Question Answering

- 질문, 본문 각각 텍스트 쌍을 입력받음.
- 본문과 질문을 입력 받으면, 본문의 일부분을 추출해서 질문에 답변



## 9. BERT Info

<hr>

- 훈련 데이터는 위키피디아(25억 단어)와 BooksCorpus(8억 단어) ≈ 33억
- WordPiece 토크나이저로 토큰화를 수행 후 15% 비율에 대해서 마스크드 언어 모델 학습
- 두 문장 Sentence A와 B의 합한 길이. 즉, 최대 입력의 길이는 512로 제한
- 100만 step 훈련 ≈ (총 합 33억 단어 코퍼스에 대해 40 에포크 학습)
- 옵티마이저 : Adam
- 학습률(learning rate) : 10^−4
- 가중치 감소(Weight Decay) : L2 정규화로 0.01 적용
- 드롭 아웃 : 모든 레이어에 대해서 0.1 적용
- 활성화 함수 : gelu 함수
- 배치 크기(Batch size) : 256



## 10. Attention Mask

<hr>

- 어텐션 연산을 할 때, 불필요한 패딩 토큰에 대해 어텐션 하지 않도록 실제 단어, 패팅 토큰 구분을 위한 입력
  - 0, 1 중 하나의 값을 가짐.
    - 1은 실제 단어
    - 0은 패딩 토큰의 위치