# Pre-training in NLP

<hr>



## 1. Pre-trained word embedding

<hr>

- 임베딩을 사용하는 방법
  - 임베딩 층을 랜덤 초기화하여 처음부터 학습하는 방법
  - 방대한 데이터로 임베딩 알고리즘(Word2Vec 등)으로 이미 학습된 임베딩 벡터를 가져와 사용하는 방법
- 임베딩을 사용하면 하나의 단어가 하나의 벡터값으로 맵핑되기 때문에 문맥을 고려하지 못함.
  - 다의어, 동음이의어 구분 못함.



## 2. Pretrained Language Model

<hr>

- LSTM 언어 모델을 학습시키고 이를 텍스트 분류에 추가하는 방법이 제안됨

  1. LSTM 언어 모델 학습

  2. 언어 모델은 주어진 텍스트에서 이전 단어들로부터 다음 단어를 예측하도록 학습

     - 별도의 레이블이 부착되지 않은 텍스트 데이터로도 학습 가능

  3. 레이블이 없는 데이터로 학습된 LSTM과 가중치가 랜덤으로 초기화 된 LSTM으로 각각 데이터를 학습하여 정확도 테스트

     - IMDB 리뷰 데이터의 경우 레이블이 없는 데이터로 학습된 LSTM이 좀 더 좋은 성능을 얻음.

       ![img](https://wikidocs.net/images/page/108730/image1.PNG)

- 방대한 텍스트로 LSTM 언어 모델을 학습(ELMo 등)

  - ELMo
    - 순방향 언어 모델, 역방향 언어 모델을 각각 학습시킨 후, 이로부터 임베딩 값을 얻음.
      - 동음이의어들 각각 임베딩 벡터 값이 달라짐.
      - Word2Vec, GloVe 같은 워드 임베딩 문제점 해결

  ![img](https://wikidocs.net/images/page/108730/image2.PNG)



## 3. Masked Language Model

<hr>

- 입력 텍스트의 단어 집합 15% 단어를 랜덤으로 마스킹
  - 마스킹 : 원래의 단어가 무엇인지 모르게 한다
- 신경망이 마스킹 된 단어 예측