# **선형 회귀(Linear Regression)**

<hr>



## 1. 선형 회귀

<hr>

- x의 값은 독립적으로 변함, y의 값은 x의 값에 의해서 종속적으로 결정
- 한 개 이상의 독립 변수 x, y의 선형 관계 모델링
- 만약, 독립 변수 x가 1개라면 **단순 선형 회귀** 라고 함.

### 1) 단순 선형 회귀 분석(Simple Linear Regression Analysis)

- 독립 변수 x, 가중치(곱해지는 값) W, 편향(별도로 더해지는 값) b

$$
y = {Wx + b}
$$

### 2) 다중 선형 회귀 분석(Multiple Linear Regression Analysis)

- y는 1개, x는 여러개

$$
y = {W_1x_1 + W_2x_2 + ... W_nx_n + b}
$$



## 2. 가설(Hypothesis) 세우기

<hr>

- 가설 : x와 y의 관계를 유추하기 위한 수학적 식
- 가설을 세워 적절한 W와 b의 값을 찾는 과정

$$
H(x) = {Wx + b}
$$

- W와 b 값에 따라 천차만별
- W는 직선의 기울기 b는 절편

![img](https://wikidocs.net/images/page/21670/W%EC%99%80_b%EA%B0%80_%EB%8B%A4%EB%A6%84.PNG)



## 3. 비용 함수(Cost function) : 평균 제곱 오차(MSE)

<hr>

- 실제값과 가설로부터 얻은 예측값의 오차를 계산하는 식을 세우고, 이 식의 값을 최소화하는 최적의 W와 b를 탐색

- 목적 함수(Objective function) : 함수의 값을 최소화하거나, 최대하하기 위한 함수

- 비용 함수(Cost function), 손실 함수(Loss function) : 값의 최소화를 위한 함수, 예측값의 오차를 줄이는데 최적화 된 식

- 회귀 문제의 비용 함수는 주로 **평균 제곱 오차(Mean Squared Error, MSE)** 사용

- 오차의 크기를 측정하기 위한 가장 기본적인 방법은 각 오차를 모두 더하는 방법

  - 음수 오차, 양수 오차를 고려하여 오차를 제곱하고 더함.

  $$
  \sum_{i=1}^{n} \left[y^{(i)} - H(x^{(i)})\right]^2
  $$
  - 이 때 데이터의 개수 n으로 나누면 오차의 제곱합에 대한 평균(MSE)

  $$
  \frac{1}{n} \sum_{i=1}^{n} \left[y^{(i)} - H(x^{(i)})\right]^2
  $$

  - W와 b에 의한 **비용 함수**로 재정의(**MSE의 값을 최소값으로 만들기 위해**)

  $$
  cost(W, b) = \frac{1}{n} \sum_{i=1}^{n} \left[y^{(i)} - H(x^{(i)})\right]^2
  $$

  - 모든 점들과의 오차가 클 수록 MSE는 커지며, 오차가 작아질 수록 MSE는 작아짐
  - Cost(W, b)를 최소가 되게 만드는 W와 b를 구하면 결과적으로 y와 x의 관계를 가장 잘 나타내는 직선을 그릴 수 있음.

  $$
  W, b → minimize\ cost(W, b)
  $$

- 



## 4. 옵티마이저(Optimizer) : 경사하강법(Gradient Descent)

<hr>

- **옵티마이저 | 최적화 알고리즘** : 비용 함수를 최소화하는 매개변수인 W와 b을 찾기 위한 알고리즘

- 기울기가 지나치게 크거나 작으면 예측 값의 오차가 커짐.

- b가 지나치게 크거나 작으면 오차가 커짐.

- 경사하강법

  - W 값이 점차 수정되어 기울기가 가장 작아진 지점을 찾는다.
  - 볼록한 부분에서 접선의 기울기는 0이 되고, 미분값 또한 0이 된다.
  - 비용 함수(Cost function)를 미분하여 현재 W, b에서의 접선의 기울기를 구하고, 접선의 기울기가 낮은 방향으로 W의 값을 변경하고 다시 미분하는 과정을 거쳐 기울기가 0인 곳을 향해 W의 값을 변경하는 작업

  ![img](https://wikidocs.net/images/page/21670/%EC%A0%91%EC%84%A0%EC%9D%98%EA%B8%B0%EC%9A%B8%EA%B8%B01.PNG)
  - 비용(cost)를 최소화하는 W, b를 구하기 위한 W를 업데이트 하는 식. **접선의 기울기가 0이 될 때까지 반복**
    - a는 학습률(learning rate), W, b의 값을 얼마나 크게 변경할지를 결정
      - a가 지나치게 높은 값을 가지면 W, b의 값이 발산하고, 지나치게 낮으면 학습 속도가 느려짐
    - 현재 W, b에서의 접선의 기울기와 a를 곱한 값을 현재 W, b에서 빼서 W, b를 갱신
    - 접선의 기울기가 음수, 양수일 때 모두 0인 방향으로 W의 값을 조정

  $$
  W := W - α\frac{∂}{∂W}cost(W) \\
  \
  b := b - α\frac{∂}{∂W}cost(b)
  $$
  - 