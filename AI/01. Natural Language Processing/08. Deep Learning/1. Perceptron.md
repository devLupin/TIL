# 퍼셉트론(Perceptron)

<hr>

- 다수의 입력으로부터 하나의 결과를 내보내는 알고리즘
- 각 입력값은 가중치가 존재하는데, 이 가중치값이 클수록 해당 입력 값이 중요하다는 것을 의미
- 편향 b는 퍼셉트론의 입력으로 사용되고 보통 입력값이 1로 고정되고 편향 b가 곱해지는 변수로 표현

![img](https://wikidocs.net/images/page/24958/perceptron2_final.PNG)

- 각 입력값이 가중치와 곱해져서 인공 뉴런에 보내지고, 각 입력값과 가중치의 곱의 합이 임계치(threshold)를 넘으면 종착지에 있는 인공 뉴런은 1을 출력
- 이러한 함수를 계단 함수(Step function)라고 함.
- 계단 함수에 사용된 임계치값을 수식으로 표현할 때 보통 세타로 표현

$$
if \sum_i^{n} W_{i}x_{i}\ ≥ \theta → y=1
\
\\
if \sum_i^{n} W_{i}x_{i}\ < \theta → y=0
\
\\
if \sum_i^{n} W_{i}x_{i} + b ≥ 0 → y=1
\
\\
if \sum_i^{n} W_{i}x_{i} + b < 0 → y=0
$$

- 초기 인공 신경망 모델인 퍼셉트론은 활성화 함수로 계단 함수 사용

  ![img](https://wikidocs.net/images/page/24987/step_function.PNG)

- **로지스틱 회귀(시그모이드 함수)**와 **퍼셉트론(계단 함수)**의 차이는 오직 활성화 함수의 차이



## 1. 단층 퍼셉트론(Single-Layer Perceptron)

- 값을 보내는 단계와 값을 받아서 출력하는 두 단계로 이뤄짐.
- 이 때 각 단계를 층(layer), 입력층(input layer), 출력층(output layer)으로 부름.

![img](https://wikidocs.net/images/page/24958/perceptron3_final.PNG)

- 단층 퍼셉트론은 직선 하나로 두 영역을 나눌 수 있는 문제에 대해서 구현 가능(선형 영역)

![img](https://wikidocs.net/images/page/24958/oragateandnandgate.PNG)

- XOR 게이트 같은 문제는 비선형 영역으로 분리해야 구현 가능

![img](https://wikidocs.net/images/page/24958/xorgraphandxorgate.PNG)



## 2. 다층 퍼셉트론(MultiLayer Perceptron, MLP)

- 입력층과 출력층 사이에 **은닉층(hidden layer) 추가**
- 은닉층이 2개 이상인 신경망을 **심층 신경망(Deep Neural Network, DNN)**

![img](https://wikidocs.net/images/page/24958/%EC%9E%85%EC%9D%80%EC%B8%B5.PNG)

- **학습(training)** : 기계가 가중치를 스스로 찾아내도록 자동화. 손실 함수, 옵티마이저 사용
- **딥러닝(Deep Learning)** : 학습 시키는 신경망이 심층 신경망인 경우