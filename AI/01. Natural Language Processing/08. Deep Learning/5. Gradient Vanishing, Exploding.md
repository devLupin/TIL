# 기울기 소실(Gradient Vanishing)과 폭주(Exploding)

<hr>

- 기울기 소실
  - 역전파 과정에서 입력층으로 갈수록 기울기가 점차적으로 작아질 때 발생
  - 입력층에 가까운 층들에서 가중치들이 제대로 업데이트가 되지 않음.

- 기울기 폭주
  - 기울기가 점차 커지더니 가중치들이 비정상적으로 큰 값이 됨.
  - 순환 신경망(Recurrent Neural Network, RNN)에서 발생



## 1. ReLU, ReLU의 변형

<hr>

- 시그모이드 함수를 사용하면 입력의 절대값이 큰 경우에 출력값이 0 또는 1에 수렴하면서 기울기가 0에 가까워짐.
- 역전파 과정에서 전파 시킬 기울기가 점차 사라져서 입력층 방향으로 갈수록 역전파가 제대로 되지않는 기울기 소실 문제 발생
- 은닉층 활성화 함수 사용(시그모이드, tanh ==> ReLU, Leaky ReLU)
  - Leaky ReLU를 사용하면 모든 입력값에 대해서 기울기가 0에 수렴하지 않아 죽은 ReLU 문제 해결



## 2. 기울기 클리핑(Gradient Clipping)

<hr>

- 기울기가 임계값을 넘지 않도록 기울기 값을 자르는 것

```python
from tensorflow.keras import optimizers
Adam = optimizers.Adam(lr=0.0001, clipnorm=1.)
```



## 3. 가중치 초기화(Weight initialization)

<hr>

### 1. Xavier 초기화

- 균등 분포(Uniform Distribution), 정규 분포(Normal distribution)으로 나뉨.
- 이전 층의 뉴런 개수와 다음 층의 뉴런 개수를 이용한 식
- 균등분포를 사용한 가중치 초기화할 경우 균등 분포 범위

$$
W \sim Uniform(-\sqrt{\frac{6}{ {n}_{in} + {n}_{out} }}, +\sqrt{\frac{6}{ {n}_{in} + {n}_{out} }})
$$

- 정규 분포 초기화인 경우 평균이 0이고 표준편차가 다음을 만족

$$
σ=\sqrt{\frac { 2 }{ { n }_{ in }+{ n }_{ out } } }
$$

- 여러 층의 기울기 분산 사이에 균형을 맞춰서 특정 층이 너무 주목 받거나 다른 층이 뒤쳐지는 것을 방지
- S자 형태 활성화 함수와 함께 좋은 성능을 보이지만, ReLU는 그렇지 못함.

### 2. He 초기화(He initialization)

- 정규 분포, 균등 분포로 나뉨.
- 다음 층의 뉴런 수를 반영하지 않음.
- 균등 분포

$$
W\sim Uniform(- \sqrt{\frac { 6 }{ { n }_{ in } } } , \space\space + \sqrt{\frac { 6 }{ { n }_{ in } } } )
$$

- 정규 분포

$$
σ=\sqrt{\frac { 2 }{ { n }_{ in } } }
$$

- 시그모이드 함수나 하이퍼볼릭탄젠트 함수를 사용할 경우에는 세이비어 초기화 방법이 효율적
- ReLU 계열 함수를 사용할 경우에는 He 초기화 방법이 효율적
- ReLU + He 초기화 방법이 좀 더 보편적



## 4. 배치 정규화(Batch Normalization)

<hr>

### 1. 내부 공변량 변화(Internal Covariate Shift)

- 학습 과정에서 **층 별로 입력 데이터 분포가 달라지는 현상**
- 이전 층들의 학습에 의해 이전 층의 가중치 값이 바뀌게 되면, 현재 층에 전달되는 입력 데이터의 분포가 현재 층이 학습했던 시점의 분포와 차이가 발생

### 2. 배치 정규화

- 배치 단위로 정규화
- 각 층에서 활성화 함수를 통과하기 전에 수행
- 입력에 대한 평균을 0으로 만들고 정규화
- 정규화 된 데이터에 대해서 스케일(γ), 시프트(β) 수행

$$
Input\ \ 미니배치\ \ B = \{{x}^{(1)}, {x}^{(2)}, ..., {x}^{(m)}\}
\
\\
Output: y^{(i)} = BN_{γ, β}(x^{(i)})
\
\\
\
\\
μ_{B} ← \frac{1}{m} \sum_{i=1}^{m} x^{(i)} \text{ # 미니 배치에 대한 평균 계산}
\
\\
\
\\
σ^{2}_{B} ← \frac{1}{m} \sum_{i=1}^{m} (x^{(i)} - μ_{B})^{2}\text{ # 미니 배치에 대한 분산 계산}
\
\\
\
\\
\hat{x}^{(i)} ← \frac{x^{(i)} - μ_{B}}{\sqrt{σ^{2}_{B}+ε}}\text{ # 정규화}
\
\\
\
\\
y^{(i)} ← γ\hat{x}^{(i)} + β = BN_{γ, β}(x^{(i)}) \text{ # 스케일 조정(γ)과 시프트(β)를 통한 선형 연산}
$$

- 학습 시 배치 단위의 평균과 분산들을 차례대로 받아 이동 평균과 이동 분산을 저장해놓았다가 테스트 할 때는 해당 배치의 평균과 분산으로 정규화

### 3.  배치 정규화의 한계

1. 미니 배치 크기에 의존적
   - 너무 작은 배치 크기에서는 분산이 적어져 훈련에 악영향
2. RNN에 적용하기 어려움
   - RNN은 각 시점마다 다른 통계치를 가지기 때문



## 5. 층 정규화(Layer Normalization)

<hr>

- 배치 정규화

![img](https://wikidocs.net/images/page/61375/%EB%B0%B0%EC%B9%98%EC%A0%95%EA%B7%9C%ED%99%94.PNG)

- 층 정규화

![img](https://wikidocs.net/images/page/61375/%EC%B8%B5%EC%A0%95%EA%B7%9C%ED%99%94.PNG)