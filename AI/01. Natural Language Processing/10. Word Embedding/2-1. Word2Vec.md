# 워드투벡터(Word2Vec)

<hr>

- 원-핫 벡터는 단어 간 유사도를 계산할 수 없다는 단점을 개선한 방법



## 1. 분산 표현(distributed representation)

<hr>

- 분포 가설이라는 가정하에 만들어진 표현법
- **분포 가설 : 비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다.**
- 벡터의 차원이 단어 집합의 크기일 필요가 없어 차원이 상대적으로 저차원으로 줄어듬.
- **단어의 의미를 여러 차원에 분산**



## 2. CBOW(Continuous Bag of Words)

<hr>

- Word2Vec는 CBOW, Skip-Gram 방식이 존재
- CBOW : 주변에 있는 단어들을 가지고, 중간에 있는 단어들을 예측하는 방법
- Skip-Gram : 중간에 있는 단어로 주변 단어들을 예측하는 방법
- 예측해야하는 단어를 중심 단어(center word), 예측에 사용되는 단어들을 주변 단어(context word)
- 윈도우(window) : 앞, 뒤로 몇개의 단어를 볼건지
- 슬라이딩 윈도우(sliding window) : 윈도우를 움직여서 주변 단어와 중심단어를 바꿔가며 학습 데이터 제작
- CBOW의 인공 신경망
  - 투사층의 크기가 M이라면, 임베딩하고 난 벡터의 차원도 M
  - 입력층과 투사층 사이의 가중치 W는 V x M 행렬
  - 투사층과 출력층 사이의 가중치 W`는 M x V 행렬
- CBOW는 주변 단어로 중심 단어를 더 정확히 맞추기 위해 W, W`를 학습해 가는 구조
  - lookup해온 W의 각 행벡터가 Word2Vec를 수행한 후 각 단어의 M차원의 크기를 갖는 임베딩 벡터
  - 
  - 스코어 벡터 : 임베딩 벡터에 softmax 함수를 취해서 0과 1의 실수 값을 얻고, 총합은 1이 되는 상태로 변경
    - j번째 인덱스가 가진 0과 1사이의 값은 j번째 단어가 중심 단어일 확률
  - 스코어 벡터 값이 원-핫 벡터의 값에 가까워지기위해 손실함수로 cross-entropy 함수 사용
  - 이후 역전파를 수행하면 W와 W`이 학습

![img](https://wikidocs.net/images/page/22660/word2vec_renew_2.PNG)



## 3. Skip-gram

<hr>

- 중심 단어에서 주변 단어 예측
- 중심 단어로 주변 단어를 예측하므로 투사층에서 벡터들의 평균을 구하는 과정이 없음.
- 전반적으로 Skip-gram이 CBOW보다 성능이 좋다고 알려져 있음.

![img](https://wikidocs.net/images/page/22660/word2vec_renew_6.PNG)



## 4. NNLM(피드 포워드 신경망 언어 모델) vs Word2Vec

<hr>

- NNLM은 언어 모델로써 다음 단어를 예측
- Word2Vec(CBOW)은 워드 임베딩 자체가 목적이므로 중심 단어를 예측하여 학습
- NNLM은 예측 단어의 이전 단어만 참고
- **Word2Vec는 예측 단어의 전, 후 단어 모두 참고**
- Word2Vec가 학습 속도에서 강점을 가지는 이유는 은닉층 제거뿐만 아니라 소프트맥스, 네거티브 샘플링 기법을 사용하기 때문

$$
NNLM : (n × m) + (n × m × h) + (h × V)
\ \\
Word2Vec : (n × m) + (m × log(V))
$$

![img](https://wikidocs.net/images/page/22660/word2vec_renew_7.PNG)