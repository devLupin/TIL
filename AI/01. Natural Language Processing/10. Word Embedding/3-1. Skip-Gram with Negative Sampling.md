# 네거티브 샘플링을 이용한 Word2Vec(Skip-Gram with Negative Sampling, SGNS)

<hr>



## 1. 네거티브 샘플링(Negative Sampling)

<hr>

- Word2Vec의 출력층에서는 소프트맥스 함수를 지난 단어 집합 크기의 벡터와 실제값인 원-핫 벡터와의 오차를 구하고 이로부터 임베딩 테이블에 있는 모든 단어에 대한 임베딩 벡터 값을 업데이트
- Word2Vec 학습 과정에서 일부 단어 집합에 집중할 수 있도록 하는 방법
- 하나의 중심 단어에 대해서 전체 단어 집합보다 훨씬 작은 단어 집합들을 만들고 마지막 단계를 이진 분류 문제로 변환
  - 주변 단어들을 positive, 랜덤으로 샘플링 된 단어들을 negative로 레이블링



## 2. 네거티브 샘플링 Skip-Gram(Skip-Gram with Negative Sampling, SGNS)

<hr>

- 중심 단어와 주변 단어가 모두 입력되고, 이들이 실제로 윈도우 크기 내에 존재하는 이웃 관계인지 그 확률을 예측
- 입력1(중심 단어)와 입력2(주변 단어 관계가 아닌 단어, 랜덤으로 선택) 각각 1과 0으로 레이블 입력

![img](https://wikidocs.net/images/page/69141/%EA%B7%B8%EB%A6%BC4.PNG)

- 중심 단어의 테이블 룩업을 위한 임베딩 테이블, 주변 단어의 테이블 룩업을 위한 임베딩 테이블 준비

  - 각 단어는 각 임베딩 테이블을 테이블 룩업하여 임베딩 벡터로 변환
  - 중심 단어와 주변 단어의 내적값을 모델의 예측값으로 설정
  - 레이블과의 오차로부터 역전파하여 중심 단어와 주변 단어의 임베딩 벡터값 업데이트

  ![img](https://wikidocs.net/images/page/69141/%EA%B7%B8%EB%A6%BC7.PNG)