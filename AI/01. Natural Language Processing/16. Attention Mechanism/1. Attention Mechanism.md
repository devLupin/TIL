# **어텐션 메커니즘 (Attention Mechanism)**

<hr>

- RNN 기반 seq2seq 모델의 문제
  - 고정된 크기의 벡터에 모든 정보를 압축하다보니 정보 손실 발생
  - RNN의 고질적인 기울기 소실(Vanishing Gradient) 문제 존재

- 텍스트 분류에서는 RNN의 마지막 은닉 상태는 예측을 위해 사용되는데 이는 몇 가지 유용한 정보를 손실한 상태임.
- 따라서 RNN이 time step을 지나며 손실했던 정보를 다시 참고하기 위해 사용



## 1. 아이디어

<hr>

- 디코더에서 출력 단어를 예측하는 매 시점(time step)마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고
  - 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아님.
  - 예측 단어와 연관 있는 입력 단어를 좀 더 어텐션해서 보는 것.



## 2. Attention Function

<hr>

- 주어진 쿼리에 대해서 모든 키와의 유사도를 구하는 함수
- 유사도를 키와 맵핑되어 있는 각각의 값에 반영
- 유사도가 반영된 값을 모두 더해서 반환(Attention Value)

```
Q = Query : t 시점의 디코더 셀에서의 은닉 상태
K = Keys : 모든 시점의 인코더 셀의 은닉 상태들
V = Values : 모든 시점의 인코더 셀의 은닉 상태들
```



## 3. Dot-Product Attention

<hr>

- 각 입력 단어가 디코더의 예측에 도움이 되는 정도가 수치화하여 측정되면 이를 하나의 정보로 담아서 디코더로 전송
- 디코더는 출력 단어를 더 정확하게 예측할 확률이 높아짐.
- 디코더 셀은 디코더 셀의 t-1 은닉상태, t-1에서 나온 출력 단어, 어텐션 값을 필요로 함.

- 수행 절차

  1. 어텐션 스코어(Attention score)를 구함.

     - 어텐션 스코어는 디코더 시점 t에서 단어를 예측

     - 인코더의 모든 은닉 상태 각각이 디코더의 현 시점의 은닉상태와 얼마나 유사한지를 판단하는 스코어 값

     - 디코더의 현 시점의 은닉상태를 전치(transpose)하고 각 은닉 상태와 내적(dot product) 수행

       - 어텐션 스코어의 값은 스칼라

       ![img](https://wikidocs.net/images/page/22893/i%EB%B2%88%EC%A7%B8%EC%96%B4%ED%85%90%EC%85%98%EC%8A%A4%EC%BD%94%EC%96%B4_final.PNG)

     - 어텐션 스코어 함수 정의

     $$
     score(s_{t},\ h_{i}) = s_{t}^Th_{i}
     $$

     - 모든 은닉 상태의 어텐션 스코어 et 수식

     $$
     e^{t}=[s_{t}^Th_{1},...,s_{t}^Th_{N}]
     $$

     ![img](https://wikidocs.net/images/page/22893/dotproductattention2_final.PNG)

  2. 소프트 맥스 함수를 통해 어텐션 분포(Attention Distribution)를 구함.

     - et에 소프트맥스 함수를 적용하여, 모든 값을 합하면 1이 되는 확률 분포
     - 각각의 값은 어텐션 가중치(Attention Wight)라고 함.

     $$
     α^{t} = softmax(e^{t})
     $$

     ![img](https://wikidocs.net/images/page/22893/dotproductattention3_final.PNG)

  3. 각 인코더의 가중치와 은닉상태를 가중합하여 어텐션 값(Attention Value)을 구함.

     - 각 인코더의 은닉 상태와 어텐션 가중치값들을 곱하고 모두 더함.

       - 가중합(Weighted Sum)을 한다와 통용

     - 어텐션 값 수식
       $$
       a_{t}=\sum_{i=1}^{N} α_{i}^{t}h_{i}
       $$

     - 어텐션 값이 인코더의 문맥을 포함할 때도 있어, 컨텍스트 벡터(context vector)라고도 불림.

     ![img](https://wikidocs.net/images/page/22893/dotproductattention4_final.PNG)

  4. 어텐션 값과 디코더의 t 시점의 은닉상태를 연결(Concatenate)

     - 어텐션 값이 구해지면 at와 st를 연결하여 벡터(vt)로 만드는 작업 수행
     - 이를 예측 연산의 입력으로 사용하며, 인코더로부터 얻은 정보를 활용하여 좀 더 나은 예측 가능

     ![img](https://wikidocs.net/images/page/22893/dotproductattention5_final_final.PNG)

  5. 출력층 연산의 입력이되는 ~st 계산

     - vt를 출력층으로 보내기 전에 신경망 연산

       - 가중치 행렬과 곱한 후 tanh 함수를 지나도록 하여 출력 층 입력을 위한 새로운 벡터(~st)를 구함.

       $$
       \tilde{s}_{t} = \tanh(\mathbf{W_{c}}[{a}_t;{s}_t] + b_{c})
       $$

     ![img](https://wikidocs.net/images/page/22893/st.PNG)

  6. ~st를 출력층의 입력으로 사용하여 예측 벡터 확보

  $$
  \widehat{y}_t = \text{Softmax}\left( W_y\tilde{s}_t + b_y \right)
  $$



## 4. Various Attention

<hr>

- 어텐션 간 차이는 중간 수식의 차이

![다양한 종류의 어텐션.PNG](https://github.com/devLupin/TIL/blob/master/AI/01.%20Natural%20Language%20Processing/%23.%20image%20src/%EB%8B%A4%EC%96%91%ED%95%9C%20%EC%A2%85%EB%A5%98%EC%9D%98%20%EC%96%B4%ED%85%90%EC%85%98.PNG?raw=true)
$$
s_{t}:Query\\
h_{i}:Keys\\
W_{a},\ W_{b}:학습\ 가능한\ 가중치\ 행렬
$$


