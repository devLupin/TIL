# 글자 임베딩(Character Embedding)

<hr>

-  단어의 벡터 표현 방법을 얻으므로서 OOV 문제를 해결
- 사람의 이해 능력을 흉내내는 알고리즘



## 1. 1D CNN을 이용한 글자 임베딩

<hr>

- 1D CNN : 전체 시퀀스 입력 내부의 더 작은 시퀀스로부터 정보를 얻어내는 동작을 하는 알고리즘
- D CNN을 글자 임베딩에 사용할 경우에는 글자의 N-gram으로부터 정보 확보

- 1D CNN을 통해서 단어 표현 벡터를 얻는 과정

  1. 글자 단위 분리
  2. 임베딩 층을 이용한 임베딩을 글자에 대해서 진행
  3. 1D CNN 적용(그림은 커널의 사이즈가 4인 커널 2개, 3인 커널 2개, 2인 커널 2개)
  4. 스칼라값들은 전부 연결(concatenate)하여 하나의 벡터로

  ![img](https://wikidocs.net/images/page/116193/%EC%BA%A1%EC%B2%981.PNG)

- 단어 벡터를 얻을 경우, 어떤 단어이든 기본적으로 글자 레벨로 쪼개므로 OOV 문제



## 2. BiLSTM을 이용한 글자 임베딩

<hr>

- 기본적으로 단어를 글자로 쪼갠 후, 임베딩 층을 사용하여 글자 임베딩을 입력으로 사용
- BiLSTM을 통해서 단어 표현 벡터를 얻는 과정
  1. 글자 단위로 분리
  2. 임베딩 층(Embedding layer)을 이용한 글자 임베딩
  3. 정방향 LSTM은 단어 정방향으로 순차적으로 글자 임베딩 벡터를 읽고. 역방향 LSTM은 단어의 역방향으로 순차적으로 글자 임베딩 읽음.
  4. 정방향 LSTM의 마지막 시점의 은닉 상태와 역방향 LSTM의 첫번째 시점의 은닉 상태를 연결(concatenate)
  5. 이렇게 얻은 벡터를 입력 단어의 벡터(글자 레벨 표현(Character-level representation))로 사용
- 워드 임베딩의 대체 또는 얻은 글자 임베딩을 워드 임베딩과 연결하여 신경망의 입력으로 사용