# RNN 언어 모델(Recurrent Neural Network Language Model, RNNLM)

<hr>



## 1. RNNLM

<hr>

- time step 개념이 도입된 RNN으로 언어 모델을 만들어 입력의 길이를 고정하지 않을 수 있다.
  - 이는 n-gram 언어 모델과 NNLM의 단점을 극복

- 기본적으로 예측 과정에서 이전 시점의 출력을 현재 시점의 입력으로 지정(**테스트 과정 동안**)
- 교사 강요(teaching forcing) : 테스트 과정에서 t 시점의 출력이 t+1 시점의 입력으로 사용되는 RNN 모델을 훈련시킬 때 사용하는 훈련 기법
  - t 시점에서 예측한 값을 t+1 시점에 입력으로 사용하지 않고, **t 시점의 레이블(정답)을 t+1 시점의 입력으로 사용**
- 훈련 과정동안 출력층에서 사용하는 활성화 함수는 소프트맥스 함수
  - 각 원소는 0과 1사이의 실수 값을 가지며 총 합은 1이 되게 하는 함수
- 모델이 예측한 값과 실제 레이블과의 오차를 계산하기 위한 손실 함수로 크로스 엔트로피 함수 사용
  - 즉, 원-핫 벡터 값에 가까워 져야하는데 실제값과 예측값을 가까워지게 하기위함
  - 이후 역전파가 이루어지면서 가중치 행렬들이 학습되고, 임베딩 벡터값들도 학습
- 임베딩 층
  - 룩업 테이블을 수행하는 투사층

- 출력층에서 나온 one-hot vector는 모델이 예측한 값의 오차를 구하기 위해 사용

![img](https://wikidocs.net/images/page/46496/rnnlm4_final.PNG)