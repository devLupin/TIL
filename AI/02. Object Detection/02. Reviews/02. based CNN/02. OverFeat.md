# OverFeat

<hr>

## 1. Multi-scale input

- multi-scale 이미지를 입력받음.
  - 하나의 이미지를 다양한 크기로 입력받으면 이미지 내 존재하는 다양한 크기의 객체를 보다 쉽게 포착 가능
  - 기존 CNN은 고정된 크기의 이미지(single-scale)을 입력받음.
    - FC layer가 고정된 크기의 feature vector를 입력받기 때문
- FC layer를 Conv layer로 대체
  - 그러나, 이로 인해 output이 가변적

## 2. Spatial outputs

- 일반적인 CNN 모델이 FC layer를 통해 최종 예측으로 1x1 크기의 output을 산출하는 경우, **non-spatial**이라고 함.
- 반면, OverFeat 모델은 detection 시, 입력 이미지의 scale에 따라 conv layer를 통해 다양한 크기의 output map 산출
  - 이를, Spatial output이라고 함.
- 원본 이미지는 nxn Pooling(stride=n) layer를 여러 번 거쳐 서로 다른 크기의 output 산출 
- 1x1 output map은 원본 이미지에 대한 정보를 **encode**
- 1x1 크기의 pixel이 encode 하는 범위를 **receptive filed**라고 함.
- **모델에 의한 산출된 spatial output의 한 요소는 원본 이미지 내 특정 receptive field에 대한 정보를 encode**
  - 이러한 정보는 학습 방법에 따라 특정 class의 confidence score나 BB의 좌표 값이 될 수 있음.
-  ex) 2x3 크기의 spatial output이 산출되면 이는 이미지 내에서 총 6개의 객체를 탐지할 수 있음을 의미

![img](https://blog.kakaocdn.net/dn/r6OIR/btqOMpYVfDX/PEfmQKkEvIsiKwwcLzr801/img.jpg)

## 3. ConvNets Sliding Window Efficiency

- **FC layer를 Conv layer로 대체하여 효율적으로 구현**
  - conv layer에서 conv filter를 적용하는 과정에서 **겹치는 영역끼리 연산을 공유**
  - 겹치는 영역에 대한 중복된 연산 회피 가능



# OverFeat Model

<hr>

- detection process
  1. 6-scale 이미지 입력
  2. classification task 목적으로 학습된 Feature extractor에 이미지를 입력하여 feature map 확보
  3. Feature map을 classifier, BB regressor에 입력하여 spatial map 출력
  4. 예측 BB에 Greedy Merge Strategy 알고리즘을 적용하여 예측 BB 출력

![img](https://blog.kakaocdn.net/dn/mr4Lo/btqPe13oMhP/pK4EWkPsTbHELwJGcdMIT0/img.jpg)

- Classification, Localization, Detection task 모두 사용 가능



### 1. OverFeat for classification Task

- AlexNet 모델과 유사하나, contrast normalization을 사용하지 않음.
- pooling 시 겹치는 영역이 없음(non-overlapping)
- 더 작은 stride를 적용하여 1, 2번째 layer의 feature map이 더 큼.



### 2. OverFeat for Localization/Detection task

- Classification task를 위해 학습된 OverFeat 모델에서 layer5까지만 사용하고 나머지 layer는 fine tuning
  - fine tuning : 이미 학습된 모델 weight로부터 모델 수정

1. Training Classifier for detection task

   - multi-scale 이미지 입력 후, 얻은 spatial output을 활용하여 detection 수행
     - feature extractor 뒤에 **FC 대신 conv layer 추가**
     - 학습 시 **1-scale** 이미지만을 사용
   - process
     1. 미리 학습시킨 OverFeat 모델을 layer 5까지만 로드하여 feature extractor로 사용하고 **layer6-8** 추가
        - **layer6(5x5 conv, 4096)**
        - **layer7(1x1 conv, 4096x4096)**
        - **layer8(1x1, 4096xC)**
     2. 이미지를 feature extractor(~layer5)에 입력하여 5x5 크기의 feature map 256개 출력
     3. 5x5 크기의 feature map을 layer 6-8에 입력하여 1x1xC 크기(C = class의 수)의 feature map 출력
     4. softmax(loss func)을 통해 학습

2. Detection by classifier

   - **Resolution Augmentation**

     - feature map이 원본 이미지에 표현하는 해상도가 너무 포괄적이게 되면 객체와 feature map 사이의 align이 맞지 않아, 모델 성능 크게 감소
     - spatial output(feature map)의 한 요소가 원본 이미지의 지나치게 넓은 receptive field를 표현하면 오히려 객체를 제대로 포착하지 못함.
     - 이를 해결하기 위해 **feature map pixel offset {0,1,2}의 조합에 따라, 총 9회의 3x3 max pooling(non-overlapping)을 수행**하는 특수한 pooling 사용
       - pixel offset 조합 : pooling을 수행하는 기준 좌표에서 x,y 방향으로 {0,1,2}만큼 shift한 좌표의 조합
       - 이는 pooling layer에 하나의 feature map이 입력되었을 때, 9개의 feature map이 출력되는 것을 의미

   - Inference(Detection)

     - **6-scale** 이미지를 입력받음.

       -  **모든 scale의 spatial output의 한 pixel은 245x245 크기의 receptive field에 대한 정보 표현**

     - process

       1. 하나의 이미지를 CNN 모델에 입력하여 layer5에서 pooling을 진행하지 않고 feature map을 얻음.
       2. pooling 되지 않은 feature map에 pixel offset의 조합에 따라 3x3 max pooling(non-overlapping) 적용
       3. 이전에 학습시킨 classifier(layer 6-8)을 거쳐 spatial output 추출
       4. spatial output은 3D output map(feature map width x height x C classes)로 reshape

       - ex) scale-2 이미지(281x317)가 입력되었을 때, Classifier 동작 과정

       ![img](https://blog.kakaocdn.net/dn/lXzUL/btqPaUX93VH/kKeV8994jqb3NWyjSoymuk/img.png)

     - spatial output을 통해 원본 이미지의 특정 receptive field에 대한 confidence score, class 출력

3. Training BB regressor

   - 학습 시, 6-scale 이미지 사용
   - 마지막 layer의 output이 4(x1, y1, x2, y2)xC(=class)가 되도록 조정
   - **ground truth box와 IoU가 0.5 미만인 예측 box는 학습에 포함시키지 않음.**
   - process
     1. (Classification task를 위해) 미리 학습시킨 Overfeat 모델을 layer5까지만 불러와 feature extractor로 사용하고 layer6-8 추가
        - **layer6(5x5 conv, 4096)**
        - **layer7(1x1 conv, 4096x4096)**
        - **layer8(1x1, 4096x4)**
     2. 이미지를 feature extractor(~layer5)에 입력하여 5x5 크기의 feature map 256개 출력
     3. 5x5 크기의 feature map을 layer 6,7,8에 입력하여 1x1x4xC(=class) 크기의 feature map 출력
     4. loss function(**L2 loss**)을 통해 학습

4. Localization by BB regressor

   - 각 **spatial map의 pixel 값은 각 class별 bounding box의 x1, y1, x2, y2 좌표**
     - **spatial output의 channel 수는 4 x C(=class)**

   ![img](https://blog.kakaocdn.net/dn/bgZzxA/btqPaV3UpBl/XtJ1PQVmYSvtn5U0MZcWoK/img.jpg)

5. Greedy Merge Strategy

   - OverFeat은 detection 시, 6-scale에 대해 굉장히 많은 예측 BB 생성
   - pixel offset 조합에 따른 pooling으로 인해 예측 BB가 9배 증가
   - 불필요한 BB를 병합하는 알고리즘
   - 프로세스
     1. 각 scale의 spatial output에 대해 각 pixel에서 가장 높은 confidence score를 가지는 class를 해당 location에 할당
     2. 해당 scale의 spatial output에 BB 좌표 할당
     3. 병합 과정 반복
   - 병합된 BB 중에서 confidence score가 가장 높은 box를 최종 예측으로 출력



- selective search를 사용하여 후보 영역을 추출한 R-CNN에 비해 detection 성능면에서 부족