# **YOLO v3(YOLO v3: An Incremental Improvement)**

<hr>



# Main Ideas

<hr>

## Bounding box Prediction

- YOLO v2는 bounding box를 예측할 때 t_x,t_y,t_w,t_h를 구한 후 그림과 같이 b_x,b_y,b_w,b_h로 변형한 후 L2 loss를 통해 학습

  - c_x, c_y는 grid cell의 좌상단 offset

- YOLO v3는 ground truth 좌표를 위의 공식을 거꾸로 적용시켜 t_∗로 변형시킨 후 t_x와 직접 L1 loss를 통해 학습

  - 예측한 BB마다 objectness score를 logistic 함수를 적용하여 구함.
  - prior box(anchor box)와 ground truth box와의 IoU 값이 가장 높은 box만 매칭
    - ground truth box에 할당되지 못한 BB는 BB regression loss를 유발하지 않고, 오직 objectness score에 대한 loss만 발생

  $$
  b_* = \sigma(t_*) + c_x \\ \sigma(t_*) = b_* - c_* \\ t_* = log(b_* - c_*)
  $$

  ![img](https://blog.kakaocdn.net/dn/sc5vS/btqXxe7GzI2/oL8SH2SltUVYPKZHj217QK/img.png)

## Class Prediction

- 각 BB는 **multi-label classification** 수행

  - 이 부분에서 softmax 함수를 이용한 class 예측은 성능이 좋지 않아, **binary cross-entropy** 사용
    - 더 복잡한 데이터셋인 경우 유의미한 결과를 보여줌.

  ![img](https://blog.kakaocdn.net/dn/8p5c3/btqXrC2A6Sv/WMBDKkgUkG0zsjVaDkHCrk/img.jpg)

## Prediction across scales

- YOLO v3는 서로 다른 3-scale을 사용하여 최종 결과 예측
- 3-scale feature map을 얻는 방법
  1. **416x416 크기의 이미지**를 네트워크에 입력하여 feature map이 크기가 **52x52, 26x26, 13x13**이 되는 layer에서 feature map 추출
  2. 가장 높은 level(해상도가 가장 낮은) feature map을 1x1, 3x3 conv layer로 구성된 **FCN(Fully Convolutional Network)**에 입력
  3. FCN의 output channel이 512가 되는 지점에서 feature map을 추출한 뒤 2배로 **upsampling** 수행
  4. 바로 아래 level에 있는 feature map과 **concatenate**
  5. merged feature map을 FCN에 입력
  6. 다음 level에 있는 feature map도 1~5 똑같이 수행
- 각 scale의 feature map의 output channel 수가 **[3 x (4 + 1 + 80)](=255)**이 되도록 마지막 1x1 conv layer의 channel 수 조정
  - 3은 grid cell당 예측하는 anchor box의 수
  - 4는 bounding box offset
  - 1은 objectness score
  - 80은 COCO 데이터셋을 사용했을 때의 class 수
- 더 높은 level의 feature map으로부터 **fine-grained 정보**를 얻을 수 있으며, 더 낮은 level의 feature map으로부터 더 유용한 **semantic 정보**를 얻을 수 있음.

## Feature Extractor

- YOLO v3에서는 shortcut connection이 추가되어 53개의 layer를 가지는 **Darknet-53을 backbone network**로 사용

  - Darkenet-53은 ResNet-101보다 1.5배 빠르며, ResNet-152와 비슷한 성능을 보이지만 2배 이상 빠름.

  ![img](https://blog.kakaocdn.net/dn/b6Ek53/btqXj64jlFd/2h5l2LXETn5OenFYh6L450/img.png)

  

# Training YOLO v3

<hr>

![img](https://blog.kakaocdn.net/dn/bNkVM3/btqXxdOyiFH/ZdEsXc4xtwS6PV7zTSRuuK/img.png)



## 1) feature map by Darknet-53

- 이미지를 입력하여 지정한 layer에서 52x52, 26x26, 13x13 크기의 feature map 추출

  

  - **Input** : 416x416 sized imaeg
  - **Process** : extract feature maps
  - **Output** : 52x52, 26x26, 13x13 sized feature maps

## 2) Building feature pyramid by FCN

- 3개의 서로 다른 scale을 가진 feature map을 1x1, 3x3 conv로 구성된 FCN에 입력하여 feature pyramid 설계

- 순서에 따라 진행되어 **52x52(x255), 26x26(x255), 13x13(x255)** 크기의 feature map을 얻음.

  

  - **Input** : 52x52, 26x26, 13x13 sized feature maps
  - **Process** : building feature pyramid by FCN
  - **Output** : 52x52(x255), 26x26(x255), 13x13(x255) sized feature maps

  ![img](https://blog.kakaocdn.net/dn/G3Llk/btqXyMiT7bQ/pQfLkTJjk6aUU8OTEd89J0/img.png)

## Train YOLO v3 by loss function

- multi-scale feature maps를 loss function을 통해 학습

- **YOLO v3의 loss function은 4개의 항으로 구성**

  1) **BB offset의 MSE(Mean Squared Error)**

  2) 객체를 예측하도록 할당된(responsible for) BB의 **objectness score의** **BCE(Binary Cross Entropy)**

  3) 객체를 예측하도록 할당되지 않은 BB의 **no objectness score의 BCE**

  4) BB의 **multi-class BCE**

- YOLO v2가 모든 예측 결과에 대하여 MSE를 적용한 반면 YOLO v3는 **BCE**를 주로 사용



# Inference

<hr>

- 마지막 예측 결과에 **NMS(Non Maximum Suppression)** 적용

- YOLO v3는 RetinaNet에 비해서는 다소 낮은 성능

- SSD와 성능이 비슷하지만 3배 이상 빠른 속도

  -  속도 면에서 혁신적인 성과

    ![img](https://blog.kakaocdn.net/dn/bgZ77Q/btqXgoYyV3h/OKkkN33ui42vuF4XkLAkz0/img.png)