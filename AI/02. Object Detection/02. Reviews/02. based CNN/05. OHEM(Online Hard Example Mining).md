# **OHEM(Online Hard Example Mining)**

<hr>

- Hard Negative Mining 방법
  - 모델이 제대로 예측하지 못한 일부 region proposals을 sampling한 후 forward, backward pass 수행
- OHEM
  - 모든 region oroposals를 forward pass한 후 loss를 계산하여, 높은 loss를 가지는 region proposals에 대해 backward pass 수행



## **1. Hard Negative Mining** 

<hr>

- region proposal 기반 classification, localization 수행 시 문제
  - 일반적으로 이미지에서 추출되는 region proposals는 객체 영역보다 백그라운드 영역이 더 많음.
  - 이로 인해, 클래스 불균형(class imbalance)문제 발생
- **모델이 잘못 예측한, 어려운(hard) sample을 추출하는 방법**
  - positive sample은 객체에 해당하는 영역, negative sample은 배경
  - 모델이 예측하기 어려운 sample은 주로 False positive sample
    - object detection task에서 고려하지 않기 때문에
  - False positive 샘플을 추출한 후 이를 학습 시킴
- Process
  1. 전체 region proposals 중에서 positive/negative sample을 적절히 섞어 mini-batch 학습
  2. hard Negative Mining 방법을 적용하여 False Positive로 판단도니 sample을 학습 데이터에 추가
  3. 이후 epoch부터 2번이 적용된 mini-batch를 입력받아 학습
  4. 반복
- False Positive를 판단하고 이를 학습 데이터셋에 추가하고, mini-batch를 구성하는 과정이 끝날 때까지 모델 업데이트 불가능, 학습이 느려짐.
- 특정 비율에 맞춰 mini-batch를 구성하는 것은 Heuristic 함.
  - 지정해줘야 하는 하이퍼 파라미터가 많아 실험자의 개입과 시행착오가 많아짐.



## **2. OHEM(Online Hard Example Mining)**

<hr>

- **이미지에서 추출한 모든 RoIs(Region of Interest)를 forward pass한 후 loss를 계산하고, 높은 loss를 가지는 RoI에 대해서만 backward pass를 수행하는 방법**
- Online은 데이터를 순차적으로 mini-batch로 구성하여 학습시키는 Online learning을 의미
- process
  1. t번째 iteration 시, 이미지를 conv layer(pre-trained VGG16)에 입력하여 feature map 추출
  2. Selective search를 통해 얻은 모든 RoI와 feature map을 사용하여 RoI pooling 수행
  3. FC layer와 Classifier, BB regressor를 거쳐 각 RoI별 loss 계산
  4. loss에 따라 RoI 정렬, B/N개의 sample만을 선택한 후 backward pass 수행
- RoI끼리 겹치는 영역의 연산이 공유되기 때문에 forward pass 시 추가되는 연산량이 상대적으로 적음.
- 적은 수의 RoI가 모델을 업데이트 하는데 사용되엇으므로 기존의 방식으로 backword pass 할 때와 연산량 차이가 적음.
- 비슷한 영역의 RoI가 backward pass시 loss가 두 번 계산될 수 있는 가능성 존재
  - Non Maximum Suppression 사용
    - RoIs와 각각의 loss가 주어질 때 NMS를 수행하여 loss가 높은 RoI와 높은 IoU 값을 가진 RoI 제거
    - 논문에서는 NMS threshold를 0.7로 지정
- 두 종류의 RoI 네트워크로 구성
  - read-only network
    - forward pass시에만 메모리 할당
    - 각 iteration마다 conv feature map이 주어지면 forward pass를 수행하고 모든 RoI에 대한 loss 계산
  - hard RoI Sampler
    - hard example만을 추출하여 일반적인 RoI network에 입력
    - 오직 hard example에 대해서만 forward/backward pass를 수행하여 gradient를 축척하여 ConvNet에 전달



## 3. Training Fast R-CNN with OHEM

<hr>

### 1) region proposal by Selective search

- 원본 이미지에 대해 Selective search 알고리즘을 적용하여 미리 region proposals 추출
  - **Input** : image
  - **Process** : Selective search
  - **Output** : region proposals

### 2) Feature extraction by pre-trained VGG16

- 훈련된 모델에 원본 이미지를 입력하여 feature map 확보
  - **Input** : image
  - **Process** : feature extraction by VGG16
  - **Output** : feature maps

### 3) max pooling by RoI pooling

- Selective search를 통해 추출한 모든 RoIs와 feature map을 사용하여 RoI pooling 수행
- 이를 통해 RoI 수만큼의 feature map 생성
  - **Input** : feature maps, **All region proposals**
  - **Process** : RoI pooling
  - **Output** : feature maps

### 4) Calculate loss by read-only RoI network

- feature map을 read-only RoI network에 입력
  - RoI network는 FC layer, BB regressor, Classfier로 구성
  - forward pass만을 수행
- 각각의 RoI에 대한 loss를 구함.
  - **Input** : feature maps
  - **Process** : Calculate loss by readonly RoI network
  - **Output** : RoI losses

### 5) Select hard examples by Hard RoI Sampler

- 중복 sample 제거를 위해 Non-Maximum-Suppression 수행
- 이후, loss를 내림차순 정렬 후 이미지별로 상위 B/N개의 RoI만 선택
  - **Input** : RoIs and RoI losses
  - **Process** : hard example sampling
  - **Output** : hard examples(mini-batch)

### 6) Max pooling by RoI pooling

- hard examples와 2)에서 얻은 feature map을 사용하여 RoI pooling 수행
- hard example 수만큼의 feature map 확보
  - **Input** : hard examples(mini-batch)
  - **Process** : RoI pooling
  - **Output** : feature maps

### 7) Train Standard RoI network

- feature map을 입력받아 FC layer, BB regressor, Classifier를 거쳐 loss를 계산한 후 backward pass를 통해 모델 학습
- **오직 hard example에 해당하는 RoI만 학습에 참여**
  - **Input** : feature maps
  - **Process** : calculate loss by Standard RoI network
  - **Output** : losses

![img](https://blog.kakaocdn.net/dn/c9M6nz/btqRnM4ET07/31cLyxkcvEBDr5ckpGohPk/img.png)



## 4) 결론

<hr>

- OHEM 방법을 적용할 경우, 학습 도중에 sampling 과정을 필요로 하지 않기 때문에 모델 학습 속도가 빠름.
  - 기존 Hard Negative Mining 방법보다 2배 빠름.
- positive/negative를 군형있게 sampling 하기 위한 별도의 하이퍼 파라미터 필요 없음.
  - 만약 특정 class의 example이 backward pass 되지 않으면, loss는 상승하고, 이는 다음 iteration 때 선택되어 backward pass될 가능성이 높아진다는 것을 의미