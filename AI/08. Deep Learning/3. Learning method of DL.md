# 딥러닝의 학습 방법

<hr>



## 1. 순전파(Foward Propagation)

<hr>

- 입력값은 입력층, 은닉층을 지나면서 각 층에서의 가중치와 함께 연산. 연산을 마치면 예측값이 나옴.
- 입력층에서 출력층 방향으로 예측값의 연산이 진행되는 과정

![img](https://wikidocs.net/images/page/36033/%EC%88%9C%EC%A0%84%ED%8C%8C.PNG)



## 2. 손실 함수(Loss function)

<hr>

- 실제값과 예측값의 차이를 수치화해주는 함수
- 손실 함수의 값을 최소화하는 두 개의 매개변수인 가중치 W와 편향 b를 찾는 것이 딥러닝의 학습 과정

![img](https://wikidocs.net/images/page/36033/%EC%86%90%EC%8B%A4%ED%95%A8%EC%88%98.PNG)

### 1) MSE(Mean Squared Error, MSE)

- 오차 제곱 평균
- 연속형 변수 예측할 때 사용(**회귀 문제**)

![img](https://wikidocs.net/images/page/24987/mse.PNG)

### 2) 크로스 엔트로피(Cross-Entropy)

- 낮은 확률로 예측해서 맞추거나 높은 확률로 예측해서 틀리는 경우 loss가 더 큼
- **분류 문제**에 사용
- 이진 분류의 경우 **'binary_crossentropy'**
- 다중 클래스 분류일 경우 **'categorical_crossentropy'**

```python
# Keras의 model.compile()

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
```



![img](https://wikidocs.net/images/page/24987/%ED%81%AC%EB%A1%9C%EC%8A%A4%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC.PNG)



## 3. 옵티마이저(Optimizer)

<hr>

### 1) 배치 경사 하강법(Batch Gradient Descent)

- 오차(loss)를 구할 때 전체 데이터를 고려
- 1 에폭에 모든 매개변수 업데이트를 단 한 번 수행
- 전체 데이터를 고려해서 학습하므로 에폭 당 시간이 오래 걸리고, 메모리를 크게 요구
- 글로벌 미니멈을 찾을 수 있음.

```python
model.fit(X_train, y_train, batch_size=len(train_X))
```

### 2) 확률적 경사 하강법(Stochastic Gradient Descent, SGD)

- **매개변수 값을 조정 시 전체 데이터가 아니라 랜덤으로 선택한 하나의 데이터에 대해서 계산하는 방법**
- 배치 경사 하강법보다 빠른 계산
- 매개변수의 변경폭이 불안정하고, 때로는 배치 경사 하강법보다 정확도가 낮을 수 있음.

```python
model.fit(X_train, y_train, batch_size=1)
```

### 3) 미니 배치 경사 하강법(Mini-Batch Gradient Descent)

- **정해진 양에 대해서만 계산하여 매개 변수의 값을 조정하는 경사 하강법**
- 전체 데이터를 계산하는 것보다 빠르며, SGD 보다 안정적
- 가장 많이 사용되는 경사 하강법

```python
model.fit(X_train, y_train, batch_size=32) #32를 배치 크기로 하였을 경우
```

### 4) 모멘텀(Momentum)

- 경사 하강법에서 계산된 접선의 기울기에 한 시점(step) 전의 접선의 기울기값을 일정한 비율만큼 반영
- 로컬 미니멈에 도달하였을 때, **기울기가 0이라서 계산이 끝났을 상황(기존의 경사 하강법)이라도, 값이 조절되면서 로컬 미니멈에서 탈출**

```python
keras.optimizers.SGD(lr = 0.01, momentum= 0.9)
```

![img](https://wikidocs.net/images/page/24987/%EB%A1%9C%EC%BB%AC%EB%AF%B8%EB%8B%88%EB%A9%88.PNG)

### 5) 아다그라드(Adagrad)

- 모든 매개변수에 동일한 learning rate를 적용하는 것은 비효율적
- 각 매개변수에 다른 학습률을 적용. 이 때, **변화가 많은 매개변수는 학습률이 작게 설정되고 변화가 적은 매개변수는 학습률을 높게 설정**

```python
keras.optimizers.Adagrad(lr=0.01, epsilon=1e-6) 	# 입실론은 분모가 0이 되지 않도록 하는 아주 작은 값
```

### 6) 알엠에스프롭(RMSprop)

- 아다그라드는 학습을 계속 진행한 경우, 점차 학습률이 떨어진다는 단점
- 이를 다른 수식으로 대체하여 단점 개선

```python
keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06) 	# rho는 이전 기울기의 영향을 감소시키는 하이퍼파라미터
```

### 7) 아담(Adam)

- 알엠에스프롭, 모멘텀을 합친듯한 방법
- 방향과 학습률을 개선하기 위한 방법

```python
keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
```



## 4. 역전파(BackPropagation)

<hr>

- 오차 값을 줄이기 위해 순전파 이후 실행
- 계산법 : https://wikidocs.net/37406



## 5. 에폭, 배치 크기, 이터레이션(Epoch, Batch size, Iteration)

<hr>

### 1) 에폭(Epoch)

- 전체 데이터에 대해서 순전파, 역전파가 끝난 상태
- 에폭 횟수가 지나치거나 적으면 과적합, 과소적합 발생

### 2) 배치 크기(Batch size)

- 몇 개의 데이터 단위로 매개변수를 업데이트 하는지
- 실제값과 예측값으로부터 오차를 계산하고 옵티마이저가 매개변수 업데이트

### 3) 이터레이션(Iteration)

- 1 에폭 내에서 이루어지는 매개변수의 업데이트 수