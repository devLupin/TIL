# 피드 포워드 신경망 언어 모델(Neural Network Language Model, NNLM)

<hr>



## 1. 기존 N-gram 언어 모델의 한계

<hr>

- 언어 모델링에 바로 앞의 n-1개의 단어만 참고
- 따라서 충분한 데이터를 관측하지 못하면 언어를 정확히 모델링하지 못하는 **희소 문제(sparsity problem)** 발생
- 언어 모델이 **단어의 유사도를 학습할 수 있도록 설계한 것**이 NNLM의 아이디어



## 2. 피드 포워드 신경망 언어 모델(NNLM)

```python
# "what will the fat cat sit on"

# 원-핫 인코딩
what = [1, 0, 0, 0, 0, 0, 0]
will = [0, 1, 0, 0, 0, 0, 0]
the = [0, 0, 1, 0, 0, 0, 0]
fat = [0, 0, 0, 1, 0, 0, 0]
cat = [0, 0, 0, 0, 1, 0, 0]
sit = [0, 0, 0, 0, 0, 1, 0]
on = [0, 0, 0, 0, 0, 0, 1]
```

- 정해진 N개의 단어만을 참고(ex. 윈도우 크기 4)

  ![img](https://wikidocs.net/images/page/45609/nnlm1.PNG)

  1. 4개의 원-핫 벡터를 입력 받은 NNLM은 투사층(projection layer)을 지난다.
     - 투사층의 특징은 가중치 행렬과의 연산은 이루어지지만 활성화 함수가 존재하지 않는다는 것
  2. 투사층의 크기를 M으로 설정하면, V x M 크기의 가중치 행렬과 곱함.
     - V는 단어 집합의 크기
     - 이 곱은 i번째 행을 그대로 읽어오는 것과 동일(룩업 테이블)

  ![img](https://wikidocs.net/images/page/45609/nnlm2_renew.PNG)

  3. 룩업 과정을 거치면  1 x M의 룩업 테이블이 N개 만들어진다.
     - 각각의 룩업 테이블은 임베딩 벡터(embedding vector)라고 함.
  4. 투사층에서  모든 임베딩 벡터들의 값 연결(concatenation)

  ![img](https://wikidocs.net/images/page/45609/nnlm3_renew.PNG)

  5. 이후 h의 크기를 가지는 은닉층을 지남.

     - 은닉층을 지난다는 것은 은늑칭의 입력은 가중치 곱해진 후 편향이 더해져 활성화 함수의 입력이 된다는 의미

  6. 은닉층의 출력은 V의 크기를 가지는 출력층으로 향함.

     - 이 과정에서 다시 또 다른 가중치와 곱해지고 편향이 더해지면, 입력이었던 원-핫 벡터와 동일한 V차원의 벡터를 얻는다.

  7.  V차원의 벡터는 출력층의 소프트맥스 함수를 지나면서 0~1 사이의 실수 값을 가지며 총 합은 1이 되는 상태로 변경

     - 이렇게 나온 벡터를 NNLM의 예측값
       $$
       \hat{y} = softmax(W_{y}h^{layer} + b_{y})
       $$

     - 1에 가까울 수록 다음 단어일 확률이 높다는 의미

     - 예측값 벡터가 실제값 벡터에 가까워지게 하기 위해 **손실 함수 cross-entropy 사용**

  8. 역전파가 이루어지면서 가중치 행렬들이 학습

     - 이 때 임베딩 벡터값들도 학습

- 충분한 훈련 데이터가 있다면 수많은 문장에서 **유사한 목적으로 사용되는 단어의 유사 임베딩 벡터값을 얻게 됨.**

- 임베딩 벡터의 아이디어는 Word2Vec, FastText, GloVe 등으로 발전

- 고정된 길이의 입력(Fixed-length input) 문제

  - **정해진 n개의 단어만을 참고하므로 버려지는 단어들이 가진 문맥 정보는 여전히 참고할 수 없다.**

