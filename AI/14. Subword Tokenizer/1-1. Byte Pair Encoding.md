# 바이트 페어 인코딩(Byte Pair Encoding, BPE)

<hr>

- 서브워드 분리의 대표적 알고리즘
- 서브워드 분리 작업은 하나의 단어는 더 작은 단위의 의미있는 여러 서브워드들의 조합으로 구성되어 있다는 아이디어로부터 시작
  - **하나의 단어를 여러 서브워드로 분리해서 단어를 인코딩 및 임베딩하는 전처리 작업**
  - 이를 통해, OOV나 희귀 단어, 신조어 같은 문제 완화



## 1. BPE(Byte Pair Encoding)

<hr>

- 연속적으로 가장 많이 등장한 글자의 쌍을 찾아서 하나의 글자로 병합하는 방식 수행

- 가장 자주 등장하는 바이트의 쌍을 다른 글자로 치환

```
aaabdaaabac

(Z=aa) 	# 가장 자주 등장한 바이트의 쌍 'aa'
ZabdZabac

(Y=ab)	# 가장 자주 등장한 바이트의 쌍 'ab'
ZYdZYac

(X=ZY) 	# 가장 자주 등장한 바이트의 쌍 'ZY'
XdXac
```



## 2. BPE in NLP

<hr>

- NLP에서 BPE는 subword segmentation(기존에 있던 단어를 분리) 알고리즘
- 글자(charcter) 단위에서 점차적으로 단어 집합을 만들어 내는 Botton-up 방식의 접근 사용
- 훈련 데이의 모든 글자 또는 유니코드 단위로 단어 집합을 만들고 가장 많이 등장하는 유니그램을 하나의 유니그램으로 통합
- refer to https://arxiv.org/pdf/1508.07909.pdf

### 1)  BPE 알고리즘

1. 딕셔너리의 모든 단어들을 글자 단위로 분리
   - 딕셔너리는 자신 도한 업데이트되며, 앞으로 단어 집합을 업데이트하기 위해 지속적으로 참고되는 참고 자료의 역할

```
# dictionary
l o w : 5,  l o w e r : 2,  n e w e s t : 6,  w i d e s t : 3

# vocabulary
l, o, w, e, r, n, w, s, t, i, d
```

2. **가장 빈도수가 높은 유니그램의 쌍을 하나의 유니그램으로 통합**(알고리즘의 동작 횟수를 사용자가 정해야 함.)

   2-1. **빈도수가 9로 가장 높은 (e, s)의 쌍을 es로 통합**

   ```
   # dictionary update!
   l o w : 5,
   l o w e r : 2,
   n e w es t : 6, 	# {n e w es t} + {w i d es t} = 6 + 3 = 9
   w i d es t : 3
   
   # vocabulary update!
   l, o, w, e, r, n, w, s, t, i, d, es
   ```

   2-2. **빈도수가 9로 가장 높은 (es, t)의 쌍을 est로 통합**

   ```
   # dictionary update!
   l o w : 5,
   l o w e r : 2,
   n e w est : 6,
   w i d est : 3
   
   # vocabulary update!
   l, o, w, e, r, n, w, s, t, i, d, es, est
   ```

   2-10. 10회 수행 후

   - 'lowest' 단어가 등장한다면, 기존에는 OOV에 해당 단어가 되지만, BPE에서는 'low'와 'est'를 찾아 인코딩
     - 두 단어가 모두 단어 집합에 있는 단어이므로 OOV가 되지 않는다.

   ```
   # dictionary update!
   low : 5,
   low e r : 2,
   newest : 6,
   widest : 3
   
   # vocabulary update!
   l, o, w, e, r, n, w, s, t, i, d, es, est, lo, low, ne, new, newest, wi, wid, widest
   ```

   ![img](https://wikidocs.net/images/page/22592/%EA%B7%B8%EB%A6%BC.png)



## 3. Wordpiece Model, WPM

<hr>

- BPE의 변형 알고리즘
- BPE가 빈도 수 기반 가장 많이 등장한 pair를 병합하는 것과 달리, 병합되었을 때 코퍼스의 Likelihood를 가장 높이는 쌍을 병합
  - 모든 단어의 맨 앞에 _를 붙이고, 단어는 서브 워드 통계에 기반하여 띄어쓰기로 분리
  - _는 문장 복원을 위한 장치
    - 기존에 없던 띄어쓰기가 추가되어 서브워드들을 구분하는 구분자 역할 수행
  - WPM이 수행되기 전으로 돌리는 방법은 현재 있는 모든 띄어쓰기를 제거하고 언더바를 띄어쓰기로 변경하면 됨.

```
WPM을 수행하기 이전의 문장: Jet makers feud over seat width with big orders at stake
WPM을 수행한 결과(wordpieces): _J et _makers _fe ud _over _seat _width _with _big _orders _at _stake
```



## 4. Unigram Language Model Tokenizer

<hr>

- 각각의 서브워드에 대한 손실(loss) 계산
  - 손실은 해당 서브워드가 단어 집합에서 제거되었을 경우, 코퍼스의 Likelihood가 감소하는 정도
- 측정된 서브워드들을 손실의 정도로 정렬하여 최악의 영향을 주는 10~20%의 토큰 제거 (이를 원하는 단어 집합의 크기에 도달할 때까지 반복)