# 과적합(Overfitting)을 막는 방법

<hr>



## 1. 데이터 양 늘리기

<hr>

- 데이터가 적은 경우, 데이터의 특정 패턴이나 노이즈까지 암기하므로 과정합 현상 발생 확률이 높아짐.
- 데이터의 양을 늘릴수록 모델은 데이터의 일반적인 패턴을 학습하여 과적합 방지
- 데이터 증식 또는 증강 : 데이터의 양이 적을 경우, 데이터를 조금씩 변형하고 추가하여 데이터의 양을 늘리기도 함.



## 2. 모델의 복잡도 줄이기

<hr>

- 인공 신경망의 복잡도는 은닉층의 수나 매개변수의 수 등으로 결정
- 모델의 수용력(capacity) : 모델에 있는 매개변수의 수



## 3. 가중치 규제(Regularization) 적용

<hr>

- L1 노름 : 가중치 w들의 절대값 합계를 비용 함수에 추가

$$
\lambda \mid w \mid
$$

- L2 노름 : 모든 가중치 w들의 제곱합을 비용 함수에 추가

$$
\frac{1}{2} \lambda w^2
$$

- λ는 규제의 강도를 정하는 하이퍼 파라미터
  - λ가 크다면 모델이 훈련 데이터에 대해서 적합한 매개 변수를 찾는 것보다 규제를 위해 추가된 항들을 작게 유지하는 것을 우선한다
- L1, L2 모두 비용 함수 최소화를 위해 가중치 w의 값이 작아져야 함.
  - L1은 가중치 w의 값들이 0또는 0에 가까이 작아져야 함.
  - L1은 어떤 특성들이 모델에 영향을 주고 있는지 정확히 판단하고자 할 때 사용. L1을 통해 어떤 가중치가 0이 되면 그 특성은 큰 영향이 없는 특성
  - L2는 0이 되기보다는 0에 가까워짐



## 4. 드롭아웃(Dropout)

<hr>

- 학습 과정에서 일부 신경망을 사용하지 않는 방법
- 학습 시에만 사용하고, 예측 시에는 사용하지 않는 것이 일반적
- 학습 시에 인공 신경망이 특정 뉴런 또는 특정 조합에 의존적이게 되는 것을 방지

```python
model = Sequential()
model.add(Dense(256, input_shape=(max_words,), activation='relu'))
model.add(Dropout(0.5)) # 드롭아웃 추가. 비율은 50%
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5)) # 드롭아웃 추가. 비율은 50%
model.add(Dense(num_classes, activation='softmax'))
```

