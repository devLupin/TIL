# 인공 신경망(Artificial Neural Network) 개요

<hr>



## 1. 피드 포워드 신경망(Feed-Forward Neural Network, FFNN)

<hr>

- 입력층에서 출력층 방향으로 연산이 전개되는 신경망

  ![img](https://wikidocs.net/images/page/24987/mlp_final.PNG)

- 순환 신경망(Recurrent Neural Network) : 은닉층의 출력값을 출력층에도 보내지만, 동시에 은닉층의 출력값이 다시 은닉층의 입력으로 사용

![img](https://wikidocs.net/images/page/24987/rnn_final.PNG)



## 2. 전결합층(Fully-connected layer, FC, Dense layer)

<hr>

- 어떤 층의 모든 뉴런이 이전 층의 모든 뉴런과 연결돼 있는 층
- 전결합층으로 구성된 피드 포워드 신경망을 전결합 피드 포워드 신경망(Fully-connected FFNN)



## 3. 활성화 함수(Activation Function)

<hr>

- 은닉층과 출력층의 뉴런에서 출력값을 결정하는 함수

### (1) 비선형 함수(Nonlinear function)

- 활성화 함수는 비선형 함수(직선 한 개로 그릴 수 없는 함수)여야 함.
- 선형 함수는 **은닉층을 여러번 추가하더라도 1회 추가한 것과 같아** 은닉층을 쌓을 수 없음.

### (2) 시그모이드 함수(Sigmoid function)와 기울기 소실

![img](https://wikidocs.net/images/page/60683/simple-neural-network.png)

- 시그모이드 함수 학습 과정
  - 입력에 대해서 순전파(forward propagation) 연산
  - 순전파 연산을 통해 나온 예측값과 실제값의 오차를 손실 함수(loss function)을 통해 계산
  - 손실(loss)을 미분해서 기울기(gradient)를 구함.
  - 역전파(back propagation) 수행
- 시그모이드 함수의 출력 값이 0 또는 1에 가까운 기울기를 계산하면 0에 가까운 아주 작은 값이 나옴.

![img](https://wikidocs.net/images/page/60683/%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C%ED%95%A8%EC%88%982.PNG)

- **기울기 소실(Vanishing Gradient)** : 역전파 과정에서 0에 가까운 기울기가 곱해지게 되면, 앞단에는 기울기가 잘 전달되지 않음.
  - 매개변수 W가 업데이트 되지 않아 학습 불가능

![img](https://wikidocs.net/images/page/60683/%EA%B8%B0%EC%9A%B8%EA%B8%B0_%EC%86%8C%EC%8B%A4.png)

- 시그모이드 함수를 은닉층에서 사용하는 것은 지양

### (3) 하이퍼볼릭탄젠트 함수(Hyperbolic tangent function)

- tanh는 -1 ~ 1 사이의 값으로 변화
- 기울기 소실 문제 발생
- 0을 중심으로 하므로 시그모이드 함수보다 반환값의 변화 폭이 커서 기울기 소실이 적은 편

![img](https://wikidocs.net/images/page/60683/%ED%95%98%EC%9D%B4%ED%8D%BC%EB%B3%BC%EB%A6%AD%ED%83%84%EC%A0%A0%ED%8A%B8.PNG)

### (4) 렐루 함수(ReLU)

- 음수를 입력하면 0을 출력하고, 양수를 입력하면 입력값을 그대로 반환

- 특정 양수값에 수렴하지 않으므로 DNN에서 잘 작동함

- 단순 임계값이므로 연산도 빠름

- 죽은 렐루(dying ReLU) : 입력값이 음수면 기울기도 0이되어 다시 회생이 매우 어려움.
  $$
  f(x) = max(0, x)
  $$
  ![img](https://wikidocs.net/images/page/60683/%EB%A0%90%EB%A3%A8%ED%95%A8%EC%88%98.PNG)

### (5) 리키 렐루(Leaky ReLU)

- 죽은 렐루를 보완하기 위한 변형 ReLU
- 입력값이 음수일 경우에 0이 아닌 0.001과 같은 매우 작은 수를 반환

$$
f(x) = max(ax, x)
$$

### (6) 소프트맥스 함수(Softmax function)

- 시그모이드 함수는 두 가지 선택지 중 하나를 고르는 이진 분류 (Binary Classification) 문제에 사용
- 세 가지 이상의 (상호 배타적인) 선택지 중 하나를 고르는 다중 클래스 분류(MultiClass Classification) 문제에 주로 사용

![img](https://wikidocs.net/images/page/60683/%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4.PNG)



## #. 행렬 크기 추정

<hr>

입력층 : 4개의 입력과 8개의 출력
은닉층1 : 8개의 입력과 8개의 출력
은닉층2 : 8개의 입력과 3개의 출력
출력층 : 3개의 입력과 3개의 출력

- 행렬의 크기 관계

$$
X_{m\ \text{×}\ n} × W_{n\ \text{×}\ j} + B_{m\ \text{×}\ j} = Y_{m\ \text{×}\ j}
$$

- layer 1의 입력 행렬 X의 크기는 1 x 4. layer 1의 출력은 8개. 그에 따라 Y의 크기는 1 x 8

$$
X_{1\ \text{×}\ 4} × W_{n\ \text{×}\ j} + B_{m\ \text{×}\ j} = Y_{1\ \text{×}\ 8}
$$

- 가중치 행렬 W의 행은 입력 행렬 X의 열과 같아야 함.

$$
X_{1\ \text{×}\ 4} × W_{4\ \text{×}\ j} + B_{m\ \text{×}\ j} = Y_{1\ \text{×}\ 8}
$$

- 편향 B는 출력 행렬 Y의 크기와 같음.

$$
X_{1\ \text{×}\ 4} × W_{4\ \text{×}\ j} + B_{1\ \text{×}\ 8} = Y_{1\ \text{×}\ 8}
$$

- 가중치 행렬 W의 열은 출력행렬 Y의 열과 동일

$$
X_{1\ \text{×}\ 4} × W_{4\ \text{×}\ 8} + B_{1\ \text{×}\ 8} = Y_{1\ \text{×}\ 8}
$$

- 출력 행렬 Y는 layer2에서 입력행렬 X가 된다.
- 은닉층과 출력층에 활성화 함수가 존재하지만 이는 행렬의 크기에 영향을 주지 않는다.