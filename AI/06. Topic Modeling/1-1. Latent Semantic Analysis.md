"""

# 토픽 모델링(Topic Modeling)

<hr>

- 문서 집합의 추상적인 주제를 발견하기 위한 통계적 모델 중 하나

- 텍스트 본문의 숨겨진 의미 구조 발견을 위해 사용되는 텍스트 마이닝 기법

# 잠재 의미 분석(Latent Semantic Analysis, LSA)

<hr>

- BoW에 기반한 DTM이나 TF-IDF는 단어의 빈도 수를 이용한 수치화 방법이기 때문에 단어의 의미를 고려하지 못함.

- 이를 위한 대안으로 DTM의 잠재된(Latent) 의미를 이끌어내는 방법

- Latent Semantic Indexing, LSI 라고 부르기도 함.

# 특이값 분해(Singular Value Decomposition, SVD)

<hr>



- SVD는 A가 m x n 행렬일 때, 행렬의 곱으로 분해(decomposition) 하는 것

$$
A=UΣV^\text{T}
$$

- $$
  U: m × m\ \text{직교행렬}\ (AA^\text{T}=U(ΣΣ^\text{T})U^\text{T})
  \\
  V: n × n\ \text{직교행렬}\ (A^\text{T}A=V(Σ^\text{T}Σ)V^\text{T})
  \\
  Σ: m × n\ \text{직사각 대각행렬}
  $$

  

### 1. 전치행렬(Transposed Matrix)

- 원래의 행렬에서 행과 열을 바꾼 행렬
- 주대각선을 축으로 반사 대칭을 하여 얻음.
- 행렬 표현의 우측 위에 t를 붙임

$$
M = 
\left[
    \begin{array}{c}
      1\ 2\\
      3\ 4\\
      5\ 6\\
    \end{array}
  \right]
\ \ \ \
M^\text{T} = 
\left[
    \begin{array}{c}
      1\ 3\ 5\\
      2\ 4\ 6\\
    \end{array}
  \right]
\ \ \ \
$$

### 2. 단위 행렬(Identity Matrix)

- 주대각선의 원소가 모두 1, 그 외는 0인 정사각 행렬
- 보통 줄여서 대문자 ***I***로 표현

$$
I = 
\left[
    \begin{array}{c}
      1\ 0\\
      0\ 1\\
    \end{array}
  \right]
\ \ \ \
I = 
\left[
    \begin{array}{c}
      1\ 0\ 0\\
      0\ 1\ 0\\
      0\ 0\ 1\\
    \end{array}
  \right]
\ \ \ \
$$

### 3. 역행렬(Inverse Matrix)

- 행렬 A와 어떤 행렬을 곱했을 때 단위 행렬이 나온다면 이를 A의 역행렬이라고 하고 A^-1 으로 표현

$$
A\ ×\ A^{-1} = I
\\
\left[
    \begin{array}{c}
      1\ 2\ 3\\
      4\ 5\ 6\\
      7\ 8\ 9\\
    \end{array}
  \right]
×
\left[
    \begin{array}{c}
      \ \ \ \ \ \ \ \ \\
      \ \ \ \ ?\ \ \ \\
      \ \ \ \ \ \ \ \ \\
    \end{array}
  \right]
=
\left[
    \begin{array}{c}
      1\ 0\ 0\\
      0\ 1\ 0\\
      0\ 0\ 1\\
    \end{array}
  \right]
$$

### 4. 직교 행렬(Orthogonal matrix)

$$
A\ ×\ A^{T} = I
\\
A^{T}\ ×\ A = I
\\
\
\\
A^{-1}=A^{T}
$$

### 5. 대각 행렬(Diagonal matrix)

- 주대각선을 제외한 곳의 원소가 모두 0인 행렬
- 대각 행렬의 주원소 a
- SVD를 통해 나온 대각 행렬은 주대각원소를 행렬 A의 특이값(singular value)이라고 함.
- 이 때 특이 값은 내림차순(big->small)으로 정렬되어 짐

$$
Σ=
\left[
    \begin{array}{c}
      a\ \ 0\ \ 0\\
      0\ \ a\ \ 0\\
      0\ \ 0\ \ a\\
    \end{array}
  \right]
  \\
  
  \\ \ \\ \ \\
  
  Σ=
\left[
    \begin{array}{c}
      a\ \ 0\ \ 0\\
      0\ \ a\ \ 0\\
      0\ \ 0\ \ a\\
      0\ \ 0\ \ 0\\
    \end{array}
  \right]
  
  \\ \ \\ \ \\
  
  Σ=
\left[
    \begin{array}{c}
      a\ \ 0\ \ 0\ \ 0\\
      0\ \ a\ \ 0\ \ 0\\
      0\ \ 0\ \ a\ \ 0\\
    \end{array}
  \right]
$$

# 절단된 SVD(Truncated SVD)

<hr>

- LSA(잠재의미분석)의 경우 풀 SVD에서 나온 3개의 행렬에서 일부 벡터들을 삭제시킨 절단된 SVD를 사용
- 대각 행렬 원소 값 중에서 상위 값 t개만 사용
- t는 찾고자하는 토픽의 수를 반영한 하이퍼파라미터 값
- t를 크게 잡으면 기존 행렬 A로부터 다양한 의미를 가져갈 수 있지만, 작게 잡아야만 노이즈를 제거할 수 있음.
- 설명력이 낮은 정보는 삭제하고, 높은 정보는 남긴다.

![img](https://wikidocs.net/images/page/24949/svd%EC%99%80truncatedsvd.PNG)



# 잠재 의미 분석(Latent Semantic Analysis, LSA)

<hr>

- 기존의 DTM, TF-IDF 행렬은 단어의 의미를 고려하지 않음.
- LSA는 DTM이나 TF-IDF 행렬에 절단된 SVD를 사용하여 차원을 축소시키고, 단어들의 잠재적인 의미를 끌어냄.
- Full SVD

```python
import numpy as np

# DTM 생성(4x9)
A=np.array([[0,0,0,1,0,1,1,0,0],[0,0,0,1,1,0,1,0,0],[0,1,1,0,2,0,0,0,0],[1,0,0,0,0,0,0,1,1]])
np.shape(A)

U, s, VT = np.linalg.svd(A, full_matrices = True) 	# full svd

print(U.round(2)) 	# 소수점 2자리까지만 출력
np.shape(s)
"""
[[-0.24  0.75  0.   -0.62]
 [-0.51  0.44 -0.    0.74]
 [-0.83 -0.49 -0.   -0.27]
 [-0.   -0.    1.    0.  ]]
 (4,)
"""

"""
	linalg.svd()는 특이값 분해의 결과로 대각 행렬이 아니라 특이값의 리스트 반환
	그러므로 이를 대각 행렬로 변경해야 함.
	특이값을 s에 저장하고 대각행렬 크기의 행렬 생성 후 특이값 삽입
"""
S = np.zeros((4, 9)) # 대각 행렬의 크기인 4 x 9의 임의의 행렬 생성
S[:4, :4] = np.diag(s) # 특이값을 대각행렬에 삽입
print(S.round(2))
np.shape(S)
"""
[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   1.73 0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]] 	# 내림차순의 양상을 보임
(4, 9)
"""

# U x S x VT를 하면 기존 행렬 A가 나와야 함.
# allclose()는 2개의 행렬이 동일하면 True 리턴
np.allclose(A, np.dot(np.dot(U,S), VT).round(2))
```

- Truncated SVD

```python
S=S[:2,:2] 	# 특이값 중 상위 2개만
U=U[:,:2]	# 2개의 열만
VT=VT[:2,:] 	# 2개의 행만

# U x S x VT 연산을 하면 기존 A와는 다른 값이 나오게 됨.
```

- 축소된 U는 문서의 개수 x 토픽의 수(t), U의 각 행은 잠재 의미를 표현하기 위한 수치화 된 각각의 문서 벡터, VT는 각 열의 잠재 의미를 표현하기 위한 수치화된 각각의 단어 벡터
- 다른 문서의 유사도, 다른 단어의 유사도, 단어(쿼리)로부터 문서의 유사도를 구하는 것이 가능



# LSA의 장단점(Pros and Cons of LSA)

<hr>

- 단어의 잠재적인 의미를 이끌어 낼 수 있어 문서의 유사도 계산 등에서 좋은 성능
- SVD의 특성상 이미 계산된 LSA에 새로운 데이터를 추가하여 계산하면 보통 처음부터 다시 계산해야 함. 즉, 새로운 정보의 업데이트가 어려움.