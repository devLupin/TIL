# 엘모(Embeddings from Language Model, ELMo)

<hr>

- 사전 훈련된 언어 모델(Pre-trained language model) 사용



## 1. ELMo(Embeddings from Language Model)

<hr>

- 같은 표기의 단어라도 문맥에 따라서 다르게 워드 임베딩을 할 수 있다면 NLP의 성능 증가
- **문맥을 반영한 워드 임베딩(Contextualized Word Embedding)** : 단어 임베딩 전에 전체 문장을 고려해서 임베딩



## 2. biLM(Bidirectional Language Model)의 사전 훈련

<hr>

- 일반적인 RNN의 경우 time-step에 따라 은닉상태의 값이 문장의 문맥 정보를 점차적으로 반영
- biLM의 경우, 순방향 RNN과 역방향 RNN 모두 활용
- 다층 구조(은닉층이 최소 2개 이상, Multi-layer)를 전제로 함.
- 워드 임베딩 방법으로 char CNN 방법 사용

- **양방향 RNN은 순방향 RNN의 은닉 상태와 역방향 RNN의 은닉 상태를 다음 층의 입력으로 보내기 전에 연결(concatenate)시킨 것**
- **biLM은 순방향 언어모델과 역방향 언어모델이 각각의 은닉 상태만을 다음 은닉층으로 보내며 훈련시킨 후에 은닉 상태를 연결시킨 것**

![img](https://wikidocs.net/images/page/33930/forwardbackwordlm2.PNG)



## 3. biLM의 활용

<hr>

- 단어 임베딩을 하기 위해 해당 시점(time-step)의 BiLM 각 층의 출력값(1. 임베딩 층, 2~. 은닉 상태)을 가져옴.
- 이후 순방향 언어 모델, 역방향 언어 모델의 각 층의 출력 값을 연결하고 추가 작업 진행
- 각 층의 출력 값이 가진 정보는 전부 서로 다른 종류의 정보를 갖고 있고, 이들 모두를 활용

- 임베딩 벡터 획득 과정

  1. 각 층의 출력값을 연결(concatenate)한다.

  ![img](https://wikidocs.net/images/page/33930/concatenate.PNG)

  2. 각 층의 출력값 별로 가중치를 부여

     ![img](https://wikidocs.net/images/page/33930/weight.PNG)

  3. 각 층의 출력 값을 모두 합함.

  ![img](https://wikidocs.net/images/page/33930/weightedsum.PNG)

  4. 벡터의 크기를 결정하는 스칼라 매개변수(γ)를 곱함.

  - 이를 ELMo 표현 이라고 함.

![img](https://wikidocs.net/images/page/33930/scalarparameter.PNG)



- ELMo 표현을 이용한 텍스트 분류 작업
  - 기존의 임베딩 벡터와 함께 사용
  - ELMo 표현을 GloVe 임베딩 벡터와 연결해서 입력으로 사용 가능
  - ELMo 표현을 만드는데 사용되는 사전 훈련된 언어 모델의 가중치는 고정
  - 가중치와 스칼라 매개변수는 훈련 과정에서 학습

![img](https://wikidocs.net/images/page/33930/elmorepresentation.PNG)