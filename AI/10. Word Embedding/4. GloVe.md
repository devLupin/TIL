# 글로브(GloVe)

<hr>

- 카운트 기반, 예측 기반을 모두 사용하는 방법론
- 기존의 카운트 기반 LSA(Latent Semantic Analysis), 예측 기반(Word2Vec) 단점을 지적하여 이를 보완한다는 목적
- **임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것**



## 1. 기존 방법의 단점

<hr>

- LSA
  - DTM, TF-IDF 행렬과 같이 각 문서에서의 각 단어의 빈도수를 카운트 한 행렬을 입력으로 받아 차원의 축소(Truncated SVD)하여 잠재된 의미 추출
  - 같은 의미(왕:남자 = 여왕:??)의 유추 작업에 성능이 떨어짐.
- Word2Vec
  - 실제값과 예측값에 대한 오차를 손실 함수를 통해 줄여나가며 학습하는 예측 기반 방법론
  - 단어 간 유추 작업에는 LSA보다 뛰어나지만, 임베딩 벡터가 윈도우 크기 내에서만 주변 단어를 고려하기 때문에 코퍼스의 전체적인 통계정보 반영 불가능



## 2. 윈도우 기반 동시 등장 행렬(Window based Co-occurrence Matrix)

<hr>

- 행과 열을 전체 단어 집합의 단어들로 구성하고, i 단어의 윈도우 크기 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬
- 전치(Transpose)해도 동일 행렬



## 3. 동시 등장 확률(Co-occurrence Probability)

<hr>

- 동시 등장 확률 P(k | i)는 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고, 특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률
- i를 중심 단어, k를 주변 단어라고 했을 때, 동시 등장 행렬에서 중심 단어 행의 모든 값을 분모로 하고 i행 k열의 값을 분자로 한 값



## 4. 손실 함수(Loss function)

<hr>

- 중심 단어, 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만듦

$$
Loss\ function = \sum_{m, n=1}^{V}\ f(X_{mn})(w_{m}^{T}\tilde{w_{n}} + b_{m} + \tilde{b_{n}} - logX_{mn})^{2}
$$



## 5. GloVe 사용

<hr>

- requirement

```
$ pip install glove_python
```



```python
from glove import Corpus, Glove

corpus = Corpus() 
corpus.fit(result, window=5)
# 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성

glove = Glove(no_components=100, learning_rate=0.05)
glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)
glove.add_dictionary(corpus.dictionary)
# 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20.

model_result1=glove.most_similar("man")
# 가장 유사한 단어 리스트 리턴
print(model_result1)
```

