# 패스트텍스트(FastText)

<hr>

- 페이스북에서 개발한 단어를 벡터로 만드는 방법
- 하나의 단어 안에도 내부단어(여러 단어들이 존재하는 것으로 간주)를 고려하여 학습



## 1. 내부 단어(subword)의 학습

<hr>

- FastText에서 각 단어는 글자 단위 n-gram의 구성으로 취급
- ex) 단어 apple 일 때, 토큰 벡터화

```
# n = 3인 경우
<ap, app, ppl, ple, le>, <apple>

# n = 3 ~ 6인 경우
<ap, app, ppl, ppl, le>, <app, appl, pple, ple>, <appl, pple>, ..., <apple>
```

- 내부 단어들을 벡터화한다는 것은 위의 단어들에 대해서 Word2Vec를 수행한다는 의미



## 2. 모르는 단어(Out Of Vocabulary, OOV)에 대한 대응

<hr>

- FastText의 인공 신경망을 학습한 후에는 데이터 셋의 모든 단어의 각 n-gram에 대해서 워드 임베딩 됨.
  - 데이터 셋이 충분한 경우, 내부 단어를 통해 모르는 단어, 다른 단어와의 유사도 계산 가능
  - Word2Vec, GloVe는 타 단어와 유사도 계산 불가능



## 3. 단어 집합 내 빈도 수가 적었던 단어(Rare Word)에 대한 대응

<hr>

- Word2Vec는 등장 빈도수가 적은 단어(rare word)의 임베딩 정확도가 높지 않음.
- FastText는 희귀 단어의 n-gram이 다른 단어의 n-gram과 겹치는 경우라면, 비교적 높은 임베딩 벡터값 획득
  - 노이즈가 많은 코퍼스에서 강점
  - 오타가 섞인 임베딩이 제대로 되지 않은 단어 또한 일정 수준의 성능을 보임.



## 4. FastText 실사용 코드

<hr>

```python
from gensim.models import FastText
model = FastText(result, size=100, window=5, min_count=5, workers=4, sg=1)

model.wv.most_similar(WORD)
```

