# **소프트맥스 회귀(Softmax Regression) - 다중 클래스 분류**

<hr>

- 3개 이상의 선택지에서 1개를 고르는 다중 클래스 분류 문제 해결 알고리즘



## 1. 다중 클래스 분류(Multi-class Classification)

<hr>

- **하나의 샘플 데이터에 대한 예측 값으로 모든 가능한 정답지에 대해서 정답일 확률의 합이 1이 됨.**
- 이를 위한 것이 **소프트맥스 함수**



## 2. 소프트맥스 함수(Softmax function)

<hr>

- 정답의 개수를 k라고 할 때 k차원의 벡터를 입력받아 각 클래스에 대한 확률 추정
- K차원의 벡터에서 i번째 원소를 zi, i번째 클래스가 정답일 확률을 pi로 나타낼 때

$$
p_{i}=\frac{e^{z_{i}}}{\sum_{j=1}^{k} e^{z_{j}}}\ \ for\ i=1, 2, ... k
$$

- k=3, z = [z1, z2, z3] 일 때, 확률 p1, p2, p3의 합은 1

$$
softmax(z)=[\frac{e^{z_{1}}}{\sum_{j=1}^{3} e^{z_{j}}}\ \frac{e^{z_{2}}}{\sum_{j=1}^{3} e^{z_{j}}}\ \frac{e^{z_{3}}}{\sum_{j=1}^{3} e^{z_{j}}}] = [p_{1}, p_{2}, p_{3}] = \hat{y} = \text{예측값}
$$

- 실제 값은 원-핫 벡터로 표현
- 예측 값과 실제 값의 오차로부터 가중치를 업데이트



## 3. 원-핫 벡터의 무작위성

<hr>

- 대부분의 다중 클래스 분류 문제가 각 클래스 간의 관계가 균등하다는 점에서 원-핫 벡터가 적절한 표현 방법임.
- 정수 인코딩과 달리 원-핫 인코딩은 분류 문제 모든 클래스 간의 관계를 균등하게 배분
- 정수 인코딩은 각 클래스가 순서나 의미를 갖고 있어 회귀를 통해서 분류 문제를 풀 수 있는 경우에 사용
  - ex) 1층~4층, 10대~40대
- 원-핫 인코딩을 통해 얻은 원-핫 벡터들은 모든 쌍에 대해서 유클리드 거리가 전부 동일
  - 이러한 무작위성은 때로는 단어의 유사성을 구할 수 없다는 단점으로 언급됨.



## 4. 비용 함수(Cost function)

<hr>

### 1. 크로스 엔트로피 함수

- y는 실제값, k는 클래스의 개수로 정의
- yj는 실제값 원-핫 벡터의 j번째 인덱스, pj는 샘플 데이터가 j번째 클래스일 확률(예측값 yj로 표현하기도 함)

$$
cost(W) = -\sum_{j=1}^{k}y_{j}\ log(p_{j})
$$

- 비용 함수의 값을 최소화하는 방향으로 학습
- n개의 전체 데이터에 대한 평균 비용 함수

$$
cost(W) = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{k}y_{j}^{(i)}\ log(p_{j}^{(i)})
$$

### 2. 이진 분류에서의 크로스 엔트로피 함수

$$
cost(W) = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{k}y_{j}^{(i)}\ log(p_{j}^{(i)}) = -\frac{1}{n} \sum_{i=1}^{n} [y^{(i)}log(p^{(i)}) + (1-y^{(i)})log(1-p^{(i)})]
$$



## 5. 소프트맥스 회귀(Softmax Regression)

<hr>

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras import optimizers

model=Sequential()
model.add(Dense(3, input_dim=4, activation='softmax'))
sgd=optimizers.SGD(lr=0.01)

model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])
# 옵티마이저는 경사하강법의 일종인 adam 사용
# 손실 함수(Loss function)는 크로스 엔트로피 함수 사용
history=model.fit(X_train,y_train, batch_size=1, epochs=200, validation_data=(X_test, y_test))
```

- **다중 클래스 분류 문제에서는 'categorical_crossentropy' 사용**(이진 분류 문제에서는 'binary crossentropy')
- validation_data 인자를 기재하면 실제로는 훈련에 반영되지 않으면서 각 훈련 횟수마다 테스트 데이터에 대한 정확도 출력